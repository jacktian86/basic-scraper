
<!DOCTYPE html PUBLIC
  "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

    
    
    
    
    


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    
        <base href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts" /><!--[if lt IE 7]></base><![endif]-->
    

    
        <meta content="2011/04/01 - " name="DC.date.valid_range" />
<link rel="kss-base-url" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/" />

  
    <link rel="stylesheet" type="text/css" href="http://crisewing.com/portal_css/crisewing.com%20Theme/base-cachekey-6a4288b1f5ff58cb412d24a5d7699aed.css" />
        <!--[if lt IE 8]>    
    
    <link rel="stylesheet" type="text/css" media="screen" href="http://crisewing.com/portal_css/crisewing.com%20Theme/IEFixes-cachekey-9a1e4208c06a6186371efdfcf3c964e1.css" />
        <![endif]-->
    
    <style type="text/css" media="all">@import url(http://crisewing.com/portal_css/crisewing.com%20Theme/resourceContentWellPortlets.stylesContentWellPortlets-cachekey-ac0e6153e4d00db59895d2ab03124ca6.css);</style>
    <link rel="stylesheet" type="text/css" media="screen" href="http://crisewing.com/portal_css/crisewing.com%20Theme/resourcejquery-ui-themessunburstjqueryui-cachekey-a018f472896b20488a2ad02a2dca9359.css" />
    <style type="text/css">@import url(http://crisewing.com/portal_css/crisewing.com%20Theme/resourcecollective.flowplayer.cssflowplayer-cachekey-1f0ec8cd6776345992478af55d55d5e2.css);</style>
    <link rel="stylesheet" type="text/css" media="all" href="http://crisewing.com/portal_css/crisewing.com%20Theme/ploneCustom-cachekey-8ee8fc1cdd61211ec52a0f8181b78ed7.css" />

  
    <link rel="kinetic-stylesheet" type="text/css" href="http://crisewing.com/portal_kss/crisewing.com%20Theme/resourcetinymce.ksstinymce-cachekey-b6bfc5c09a03b431f201c86a1359995a.kss" />
    <link rel="kinetic-stylesheet" type="text/css" href="http://crisewing.com/portal_kss/crisewing.com%20Theme/at-cachekey-cc70007125bd046aa0c600b617cb1d7e.kss" />
  
    <script type="text/javascript" src="http://crisewing.com/portal_javascripts/crisewing.com%20Theme/jquery-cachekey-3d3aee2c96dd957234da67d45127b287.js"></script>
       <!--[if lt IE 8]>
     
    <script type="text/javascript" src="http://crisewing.com/portal_javascripts/crisewing.com%20Theme/iefixes-cachekey-42597b17d0273334e8607aa5425f69e4.js"></script>
       <![endif]-->
     
    <script type="text/javascript" src="http://crisewing.com/portal_javascripts/crisewing.com%20Theme/collective.js.jqueryui.custom.min-cachekey-47bebbba21dbd85bd82204b3588251c6.js"></script>


<title>Open Source Posts &mdash; CrisEwing.com</title>
        

    <link rel="shortcut icon" type="image/x-icon" href="http://crisewing.com/favicon.ico" />
    <link rel="apple-touch-icon" href="http://crisewing.com/touch_icon.png" />


<link rel="alternate" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/atom.xml" title="Atom 2005" type="application/atom+xml" />
<link rel="alternate" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/feed.rdf" title="RDF 1.0" type="application/rdf+xml" />
<link rel="alternate" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/feed11.rdf" title="RDF 1.1" type="application/rdf+xml" />
<link rel="alternate" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/rss.xml" title="RSS 1.0" type="application/rss+xml" />
<link rel="alternate" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/itunes.xml" title="RSS 2.0" type="application/rss+xml" />

<script type="text/javascript">
        jQuery(function($){
            $.datepicker.setDefaults(
                jQuery.extend($.datepicker.regional[''],
                {dateFormat: 'mm/dd/yy'}));
        });
        </script>

    <link rel="home" href="http://crisewing.com" title="Front page" />

    <link rel="contents" href="http://crisewing.com/sitemap" title="Site Map" />






    <link rel="search" href="http://crisewing.com/search_form" title="Search this site" />



        
        
        
        
        

        <meta name="viewport" content="width=device-width; initial-scale=0.6666; maximum-scale=1.0; minimum-scale=0.6666" />
        <meta name="generator" content="Plone - http://plone.org" />
    
</head>

<body class="template-full_feed portaltype-collage site-Plone section-cover" dir="ltr">

<div id="visual-portal-wrapper">

        <div id="portal-top" class="row">
<div class="cell width-full position-0">
            <div id="portal-header">
    <p class="hiddenStructure">
  <a accesskey="2" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/full_feed#content">Skip to content.</a> |

  <a accesskey="6" href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/full_feed#portal-globalnav">Skip to navigation</a>
</p>

<div id="portal-personaltools-wrapper">

<h5 class="hiddenStructure">Personal tools</h5>



<dl class="actionMenu deactivated" id="portal-personaltools">
  <dt id="anon-personalbar">
    
        <a href="http://crisewing.com/login" id="personaltools-login">Log in</a>
    
  </dt>
</dl>

</div>



<div id="portal-searchbox">
    <form name="searchform" id="searchGadget_form" action="http://crisewing.com/search">

        <div class="LSBox">
        <label class="hiddenStructure" for="searchGadget">Search Site</label>

        <input name="SearchableText" type="text" size="18" title="Search Site" accesskey="4" class="searchField inputLabel" id="searchGadget" />

        <input class="searchButton" type="submit" value="Search" />

        <div class="searchSection">
            <input id="searchbox_currentfolder_only" class="noborder" type="checkbox" name="path" value="/Plone/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3" />
            <label for="searchbox_currentfolder_only" style="cursor: pointer">
                only in current section
            </label>
        </div>

        <div class="LSResult" id="LSResult" style=""><div class="LSShadow" id="LSShadow"></div></div>
        </div>
    </form>

    <div id="portal-advanced-search" class="hiddenStructure">
        <a href="http://crisewing.com/search_form" accesskey="5">
            Advanced Search&hellip;
        </a>
    </div>

</div>

<a id="portal-logo" title="CrisEwing.com" accesskey="1" href="http://crisewing.com">
    <img src="http://crisewing.com/logo.png" alt="CrisEwing.com" title="CrisEwing.com" height="63" width="238" /></a>


    <h5 class="hiddenStructure">Sections</h5>

    <ul id="portal-globalnav"><li id="portaltab-index_html" class="selected"><a href="http://crisewing.com" title="">Home</a></li><li id="portaltab-dev" class="plain"><a href="http://crisewing.com/dev" title="What can we do for you?">Development</a></li><li id="portaltab-edu" class="plain"><a href="http://crisewing.com/edu" title="What do you want to learn?">Training</a></li><li id="portaltab-about" class="plain"><a href="http://crisewing.com/about" title="Who am I?">About</a></li><li id="portaltab-blog" class="plain"><a href="http://crisewing.com/blog" title="Writings about open source software, programming, teaching and life in general">Blog</a></li></ul>

</div>


    <div id="portlets-in-header" class="row">
         
         
    </div>

    


</div>
        </div>
    <div id="portal-columns" class="row">

        <div id="portal-column-content" class="cell width-full position-0">

            <div id="viewlet-above-content"><div id="portal-breadcrumbs">

    <span id="breadcrumbs-you-are-here">You
are here:</span>
    <span id="breadcrumbs-home">
        <a href="http://crisewing.com">Home</a>
        
    </span>

</div>

<div id="portlets-above" class="row">
    
    
</div>

</div>

            
                <div class="">

                    

                    

    <dl class="portalMessage info" id="kssPortalMessage" style="display:none">
        <dt>Info</dt>
        <dd></dd>
    </dl>



                    
                        <div id="content">

                            

                            

    

    <h1 class="documentFirstHeading">Open Source Posts</h1>

    


    

    <div class="feedEntry">

        <a href="http://blog.2ndquadrant.com/postgresql-no-tablespaces-on-ramdisks/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=postgresql-no-tablespaces-on-ramdisks" title="Craig Ringer: Putting a PostgreSQL tablespace on a ramdisk risks ALL your data">
            <h2>Craig Ringer: Putting a PostgreSQL tablespace on a ramdisk risks ALL your data</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 15, 2014</span>.
            
        </p>

        <p>I periodically see people being advised to <a href="http://magazine.redhat.com/2007/12/12/tip-from-an-rhce-memory-storage-on-postgresql/">put their tablspaces on RAM disks or tempfs volumes</a>. This is very bad advice. <a href="http://thebuild.com/blog/2013/03/10/you-cannot-recover-from-the-loss-of-a-tablespace/">Do <b>not</b> put a PostgreSQL <tt>TABLESPACE</tt> on a RAM disk or tempfs</a>. </p>
<h2>Why you shouldn&#8217;t put a tablespace on a ramdisk</h2>
<p>Unlike MySQL and some other databases, PostgreSQL tablespaces are not completely independent of the rest of the database system.  You can&#8217;t just throw a tablespace away and have the rest of the database system keep on working.</p>
<p>Most importantly, the write-ahead log (WAL) that provides PostgreSQL&#8217;s crash safety is stored in <tt>pg_xlog</tt>, and is shared across all tablespaces. PostgreSQL expects to be able to <i>replay</i> this log in order and without errors after a crash or shutdown. Until the log has replayed, the database is assumed to be in an unsafe state and connections will be refused with:</p>
<pre>
FATAL:  the database system is starting up
</pre>
<p>If the write-ahead log contains changes to tables/indexes/etc in a tablespace that no longer exists, PostgreSQL cannot replay the WAL. It will get stuck, and will never start up successfully. You will be unable to access any of your tables on any database in that PostgreSQL instance.</p>
<p>PostgreSQL can&#8217;t just throw them away, because it doesn&#8217;t know you meant to remove the tablespace. What if that&#8217;s your tablespace for vitally important accounting data on a separate encrypted hard drive, and the admin hasn&#8217;t entered the password required to mount the drive yet? Or, more likely, what if you moved it and forgot to update the tablespace symlink? You don&#8217;t want PostgreSQL saying &#8220;oh, I guess you didn&#8217;t want that data after all&#8221;.</p>
<p>So. <i>Don&#8217;t put a tablespace on a ramdisk</i>.</p>
<p>The same problem affects removable storage. It&#8217;s fine to put a tablespace on removable storage, but you can&#8217;t remove the storage until you first drop the tablespace.</p>
<h2>How this could improve</h2>
<p>There are a few improvements we could make to PostgreSQL in this area. I&#8217;ve already helped make one of them: 9.4 now <a href="http://www.postgresql.org/docs/devel/static/manage-ag-tablespaces.html">has a warning about this admin error in the documentation</a>.</p>
<p>One useful, albeit platform specific option, would be to try to detect temporary storage and warn the user. It would be ultimately pretty futile, though &#8211; what about removable storage? What if the user creates the tablespace on durable storage then moves the symlink?  What about a SAN volume on non-durable storage? etc. It&#8217;d be at best an attempt to catch obvious admin mistakes. It might still be useful, but it&#8217;s not clear that it&#8217;s worth adding the complexity required to implement it.</p>
<p>Instead, we need two things:</p>
<ul>
<li>An option that can be used to recover from this mistake. Akin to <tt>zero_damaged_pages</tt>, the (currently nonexistent) <tt>discard_missing_tablespaces</tt> option would throw away all WAL writes destined to tablespaces that do not currently exist. It would have to mark the tablespace as broken in the system catalogs so all attempts to access it failed. The admin could restart PostgreSQL with this option enabled to recover from an error like this, or could even run with it always enabled if they regularly used tablespaces on temporary storage</li>
<li>A <tt>TEMPORARY TABLESPACE</tt> feature that marks a tablespace as disposable. If PostgreSQL sees WAL writes to a tablespace marked TEMPORARY and it doesn&#8217;t exist, it would discard the writes and drop the tablespace.</li>
</ul>
<p>I&#8217;ll write about options for recovering databases where you&#8217;ve discarded/lost an in-use tablespace in a follow-up post, along with some suggestions on what you can do safely as an alternative. In the mean time, <a href="http://stackoverflow.com/a/9407940/398670">here&#8217;s a Stack Overflow post I wrote a while ago about how to improve PostgreSQL&#8217;s performance for unit/integration testing without relying on ramdisks</a>.</p>
    </div>
    <div class="feedEntry">

        <a href="http://adpgtech.blogspot.com/2014/06/json-in-postrgesql-94-video.html" title="Andrew Dunstan: JSON in PostgreSQL 9.4 Video">
            <h2>Andrew Dunstan: JSON in PostgreSQL 9.4 Video</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 15, 2014</span>.
            
        </p>

        Here is the video presentation of my <a href="https://www.youtube.com/watch?v=MmzbnMqBMI0">pgcon talk on JSON in PostgreSQL 9.4</a>. All the other pgcon presentations can be seen nearby, too.
    </div>
    <div class="feedEntry">

        <a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-gist-inet-datatype/" title="Michael Paquier: Postgres 9.4 feature highlight: GiST operator class for inet and cidr datatypes">
            <h2>Michael Paquier: Postgres 9.4 feature highlight: GiST operator class for inet and cidr datatypes</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 15, 2014</span>.
            
        </p>

        <p>Postgres 9.4 is adding a new in-core GiST operator class for the <a href="http://www.postgresql.org/docs/9.4/static/datatype-net-types.html#DATATYPE-INET">inet and cidr</a>
datatypes. It has been introduced by this commit:</p>
<div class="highlight"><pre><code class="text language-text">commit f23a5630ebc797219b62797f566dec9f65090e03
Author: Tom Lane &lt;tgl@sss.pgh.pa.us&gt;
Date:   Tue Apr 8 15:46:14 2014 -0400

Add an in-core GiST index opclass for inet/cidr types.

This operator class can accelerate subnet/supernet tests as well as
btree-equivalent ordered comparisons.  It also handles a new network
operator inet &amp;&amp; inet (overlaps, a/k/a &quot;is supernet or subnet of&quot;),
which is expected to be useful in exclusion constraints.

Ideally this opclass would be the default for GiST with inet/cidr data,
but we can't mark it that way until we figure out how to do a more or
less graceful transition from the current situation, in which the
really-completely-bogus inet/cidr opclasses in contrib/btree_gist are
marked as default.  Having the opclass in core and not default is better
than not having it at all, though.

Emre Hasegeli, reviewed by Andreas Karlsson, further hacking by me
</code></pre></div>
<p>Note that the default operator for this data type remains btree for
historical reasons and that the new GiST operator needs to be specified
as follows using inet_ops:</p>
<div class="highlight"><pre><code class="text language-text">=# CREATE TABLE aa (a inet);
CREATE TABLE
=# CREATE INDEX aai ON aa(a);
CREATE INDEX
=# CREATE INDEX aai2 ON aa USING gist (a inet_ops);
CREATE INDEX
=# \d aa
     Table &quot;public.aa&quot;
 Column | Type | Modifiers 
--------+------+-----------
 a      | inet | 
Indexes:
    &quot;aai&quot; btree (a)
    &quot;aai2&quot; gist (a inet_ops)
</code></pre></div>
<p>By looking at <a href="http://www.postgresql.org/docs/9.4/static/functions-net.html">the documentation listing all the operators available for
those datatypes</a>
(as well as at the commit log on top of this post), the major addition that
this feature brings is a new operator &amp;&amp; to test if an inet address contains
of is contained by a second one. Here is what happened in 9.3 and older
versions because of the lack of operator:</p>
<div class="highlight"><pre><code class="text language-text">=# SELECT inet '145.127.4.84/24' &amp;&amp; inet '145.127.4.81/24' as res;
ERROR:  42883: operator does not exist: inet &amp;&amp; inet
LINE 1: SELECT inet '145.127.4.84/24' &amp;&amp; inet '145.127.4.80/32';
                                      ^
HINT:  No operator matches the given name and argument type(s). You might need to add explicit type casts.
LOCATION:  op_error, parse_oper.c:722
</code></pre></div>
<p>And here is how the solution has evolved with 9.4:</p>
<div class="highlight"><pre><code class="text language-text">=# SELECT inet '145.127.4.84/24' &amp;&amp; inet '145.127.4.81/24' as res;
 res 
-----
 t
(1 row)
</code></pre></div>
<p>A small thing perhaps, but always useful to know.</p>
    </div>
    <div class="feedEntry">

        <a href="http://momjian.us/main/blogs/pgblog/2014.html#June_14_2014" title="Bruce Momjian: Postgres Pool Party">
            <h2>Bruce Momjian: Postgres Pool Party</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 14, 2014</span>.
            
        </p>

        <p>If you are near Philadelphia, you are invited to attend the 2014 Postgres Pool Party at my home:
</p>
<ul>
  <li><strong>When:</strong>  Saturday, July 26, 2pm to 7pm
  </li><li><strong>Where:</strong>  my home in Newtown Square, Pennsylvania (<a class="txt2html" href="http://momjian.us/main/directions.html" style="text-decoration: none;">directions</a>)
  </li><li><strong>What:</strong>  barbecue, swimming, and good conversation
</li></ul>
<p>All Postgres users, developers, and groupies are invited, including their families.  Please RSVP via
<a class="txt2html" href="http://momjian.us/main/contact.html" style="text-decoration: none;">email</a> by July 20.
</p>
    </div>
    <div class="feedEntry">

        <a href="http://pgsnaga.blogspot.com/2014/06/deploying-postgres-xl-in-2-minutes-with.html" title="Satoshi Nagayasu: Deploying Postgres-XL in 2-minutes with Chef/serverspec">
            <h2>Satoshi Nagayasu: Deploying Postgres-XL in 2-minutes with Chef/serverspec</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 14, 2014</span>.
            
        </p>

        As you may know, Postgres-XL, a MPP implementation of PostgreSQL, was releasedÂ  last month.

Postgres-XL | Open Source Scalable SQL Database Cluster http://www.postgres-xl.org/

Most of recent topics in the PostgreSQL development are related to implementing data warehouse. And Postgres-XL is getting attention.

However, such enhancement which consists of several PostgreSQL servers is a bit
    </div>
    <div class="feedEntry">

        <a href="http://gorthx.wordpress.com/2014/06/13/postgres-monitoring-wishlist/" title="gabrielle roth: Postgres Monitoring Wishlist">
            <h2>gabrielle roth: Postgres Monitoring Wishlist</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 13, 2014</span>.
            
        </p>

        Since &#8220;what should I monitor in my database&#8221; has come up in conversation several times lately, I thought I&#8217;d put this here where I (theoretically) won&#8217;t lose it. I&#8217;ll save for later the discussion of where to get this info and which tools give me which stats :) Bare minimum: server CPU, memory, I/O, network [&#8230;]<img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=gorthx.wordpress.com&#038;blog=32848600&#038;post=932&#038;subd=gorthx&#038;ref=&#038;feed=1" width="1" />
    </div>
    <div class="feedEntry">

        <a href="http://www.databasesoup.com/2014/06/pending-portland-postgres-presentations.html" title="Josh Berkus: Pending Portland Postgres Presentations">
            <h2>Josh Berkus: Pending Portland Postgres Presentations</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 13, 2014</span>.
            
        </p>

        This year is apparently my year to spend most of my time in Portland.&nbsp; Maybe I should move there ;-)&nbsp; Please read through if you are in Portland or are attending conferences in Portland for a list PostgreSQL-related events there.<br /><br />O'Reilly OSCON 2014: on Monday, July 21st, <a href="http://www.oscon.com/oscon2014/public/schedule/detail/34026" target="_blank">I will be presenting The Accidental DBA again at OSCON</a>.&nbsp; O'Reilly has asked me to reprise this tutorial as it was sold out last year.&nbsp; This tutorial covers the care and feeding of a PostgreSQL server for people whose main job is something else, or for those new to database administration.&nbsp; Will include multiple hands-on exercises using Vagrant on your own laptop.&nbsp; The <a href="https://github.com/pgexperts/accidentalDBA" target="_blank">exercises are on github</a>, but will be updated on or before July 10th in order to cover 9.3 and feedback from the last talk.<br /><br />Note that there will also be a special PDXPUG meeting or BOF at OSCON; details TBD.&nbsp; I will also be doing an Office Hours session at OSCON on July 22nd at 3:20 PM; please bring your questions about PostgreSQL scalability and replication.<br /><br />I am likely to be <a href="http://www.djangocon.us/" target="_blank">speaking at DjangoCon.us</a> in September.&nbsp; As the Call for Presentations is still open, I don't know what it will be about, or have confirmation that I will, in fact, be speaking. <br /><br />Right between DjangoCon.us and FOSS4G, <a href="http://wiki.postgresql.org/wiki/PDXPUGDay2014" target="_blank">PDXPUG will be hosting a full PostgreSQL Day</a> at Portland State University on September 6.&nbsp; This event is <u><i>free</i></u>, so if you are going to either DjangoCon or FOSS4G, extend your stay in Portland in order to attend!&nbsp; RSVP required.<br /><br />People are still submitting presentations for that; if you're interested in speaking, contact Mark per the wiki page.&nbsp; I expect to be talking about 9.4, replication, or maybe Postgres on Amazon.<br /><br />Finally, <a href="https://2014.foss4g.org/" target="_blank">at FOSS4G</a>, I will be doing <a href="https://2014.foss4g.org/schedule/workshops/" target="_blank">a full day of Postgres/PostGIS workshops</a>.&nbsp; In the morning I will do Accidental DBA again, this time tailored for PostGIS administrators.&nbsp; In the afternoon, I will do a hands-on "learning to do PostGIS replication" tutorial.&nbsp; This does mean that I will likely not include much about replication in the morning session.&nbsp; Both of these workshops are "bring your own device", which means that you will need to install the tutorial materials on your laptop in advance.<br /><br />Since I will be spending the first half of September in Portland.&nbsp; If  you or your company is interested in onsite training or consulting  during September, <a href="http://pgexperts.com/" target="_blank">please get in touch with PGX</a>.
    </div>
    <div class="feedEntry">

        <a href="http://shisaa.jp/postset/postgis-postgresqls-spatial-partner-part-1.html" title="Tim van der Linden: Postgis, PostgreSQL's spatial partner - Part 1">
            <h2>Tim van der Linden: Postgis, PostgreSQL's spatial partner - Part 1</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 12, 2014</span>.
            
        </p>

        <h3>Preface</h3>
<p>In Dutch we have an expression that says "Van hier tot Tokio", which literally translated means "From here to Tokyo" and is used to indicate that something is <em>very</em> far or <em>very</em> difficult.
Unless you live in Japan, like me, then Tokyo is not <em>that</em> far actually....but you get the point. Tokyo is far, period.</p>
<p>But the question for today is...<em>how</em> far is it <em>exactly</em>? From where you are reading this right now...how far is Tokyo from you? How can you know?
You could of course just hop online and question your favorite search engine for help, or use something like Open Street Map to figure it out.</p>
<p>But that would be too simply, no? This would mean my post has to stop here, and, as some of you might know, it is difficult for me to write short blog posts. Sorry.</p>
<p>Also, you would miss out on all of the fun that is actually happening behind the screen when you question spatial search engines and that is against my belief: <em>know how the tools you depend on actually work</em>!</p>
<p>And, as the same some of you might know, I love PostgreSQL.</p>
<p>So, knowing that I cannot write short posts <em>and</em> I like PostgreSQL...what would you suspect would happen if you ask me how far Tokyo is from my current location?
You guessed it, simply use The Elephant to figure that out!</p>
<p>As I have showed you <a href="http://shisaa.jp/postset/postgresql-full-text-search-part-1.html" title="PostgreSQL full text search, chapter one.">before</a>, PostgreSQL is capable of storing, matching and retrieving much more then boring VARCHAR or INT data types and it is designed to be extendable.
And extending is what the folks behind the <em>PostGIS</em> project did. To summarize, the PostGIS project extends PostgreSQL to store, match, manipulate and retrieve <em>spatial</em> data. It makes PostgreSQL a full-blown GIS.</p>
<p>The purpose of this series is to get your feet wet with PostGIS and to learn a thing or two about GIS itself.
In the first chapter, the one you are reading now, I would like to show you some fundamental GIS concepts: GIS Objects, standardization of GIS, geography and projections. 
We will not be doing any database action today I am afraid.</p>
<p>Then, starting from the second chapter, we will open up PostgreSQL, initiate a database to be PostGIS aware and start playing around.
We will look at a bunch of different database functions we have available and how the knowledge from this chapter maps to the actual database.
And we will of course be solving the question posed above: how far is Tokyo from your current location. </p>
<p>Are you ready for a new PostgreSQL adventure?</p>
<p><em>Note:</em> I will take you over all the following information in lighting speed. 
My intent is not to make you a GIS expert, but I do feel it is necessary to touch on a few important topics so you know why PostGIS is doing stuff the way it does.
This will hopefully make the actual database work from the next chapter more clear and spark some curiosity towards learning more about this topic.</p>
<h3>The data</h3>
<p>Before we can do anything GIS related, we need to take a look at what kind of data we will be working with: the GIS objects.</p>
<h4>GIS Objects?</h4>
<p>Geographic information system, or GIS in short, is merely the name of any system which can store, retrieve, generate, manipulate and visualize spatial data - the kind of data that represents objects in two or three dimensional space.</p>
<p>The GIS world is a world of standards, as with most computer sciences. These standards define what spatial data is and how we can work with it and is defined and maintained by the Open Geospatial Consortium or <em>OGC</em>.</p>
<p>Every system that wishes to work with GIS data, including PostGIS, should adhere to these standards.</p>
<h4>Simple Features</h4>
<p>The OGC's standard for working with GIS data in SQL is defined in a OGC and <em>ISO</em> specification called <em>Simple Features</em>.</p>
<p>Simple Features defines how we can represent spatial objects, as you will see soon, but also defines how we can access and manipulate them.
You typically have available:</p>
<ul>
<li>Functions to <em>create</em> two dimensional spatial objects</li>
<li>Functions to <em>alter</em> these objects</li>
<li>Functions to <em>retrieve</em> and <em>describe</em> single or multiple objects</li>
<li>Functions to <em>compare</em> and <em>measure</em> single or multiple objects</li>
</ul>
<p>PostGIS has been certified by the OGC for its wide support of the Simple Features set.</p>
<h4>Well-known Text</h4>
<p>The first and most important part that is defined in the Simple Features spec are the means by which we can represent spatial data.
I mean, we know how we can represent numbers or strings of text inside our database, but how do we represent something more abstract as a line, or a square?</p>
<p>Folks familiar with 2D drawing or 3D modeling software might already have a gut feeling of how to represent such data, and this gut feeling is right: you store coordinates.
If you wish to represent a line, you will only need to know the two end points of this line to be able to store, manipulate or visualize it.
The same goes for a square, though there you will need four coordinates.</p>
<p>And, as is always the case with standards, the OGC has devised two famous ways of representing these objects and their coordinates: Well-known Text or <em>WKT</em> and Well-know Binary or <em>WKB</em>.
These two are almost identical, only differing in the area of use.</p>
<p>WKT is a markup language which you can use to simply write down your objects and use it in queries. It is human readable.
However, if you wish to store it in a database or wish to perform matches on the data, it has to be stored in a defined binary format, the WKB format that is.</p>
<p>WKT can represent a wide range of objects from simple points to complex multi-polygons. The notation, however, stays roughly the same.
If you wish to represent a square, for example, you could use the <em>POLYGON</em> object:</p>
<div class="code"><pre><span class="n">POLYGON</span> <span class="p">((</span><span class="mi">4</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>


<p>For people unfamiliar with the term "polygon", a polygon is a <em>closed</em>, <em>two dimensional</em> object with only <em>straight</em> lines. It has to have a minimum of three coordinates (points) thus giving it a minimum of three straight edges (making it, in that case, a triangle).</p>
<p>Let us take a deeper look at what is happening here. First, you will see we define a polygon object which you will need if you wish to represent closed, shape objects.
Next we define the four coordinates, the four corners of our square, laid out on a fictional grid of 4 by 4 units. There are two important notes to take about this coordinate listing:</p>
<p>First, the coordinates are all two dimensional and represent and X and a Y coordinate respectively.</p>
<p>Also, you do not see four but <em>five</em> coordinates. This is another rule from the spec that tells us that all polygon shapes <em>must</em> be closed.
To get a better visualization of this you could imagine a pen moving to each coordinate. To finish the loop you draw, the pen has to move back to the original coordinate.</p>
<p>The last thing to note is that the drawing direction of these coordinates is <em>counterclockwise</em>, as is with most computer defined drawing systems.
This means we put our pen on our grid at coordinate (4 1) and then draw <em>up</em> in a straight line to (4 4). Next we go <em>left</em> in a straight line to (1 4) and <em>down</em> in a straight line to (1 1).
Finally, we close the loop by drawing a straight line <em>right</em>, to the starting coordinate (4 1).</p>
<p>It is also perfectly possible to define more then one coordinate set when defining a polygon object.
A definition like this is perfectly legal:</p>
<div class="code"><pre><span class="n">POLYGON</span> <span class="p">((</span><span class="mi">8</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>


<p>This will create a square polygon with a size of 8 by 8, called the <em>exterior ring</em> and another square inside it with a size of 4 by 4.
Because this small square resides <em>inside</em> the area of the big square we call it the <em>interior ring</em> and, as a result, this small square will be interpreted by the standard as a hole in the bigger square.</p>
<p>To bring this even further, you can define as many holes in your exterior ring as you like, you simply have to make sure that the interior rings never touch each other and never go outside of the exterior ring.
The exterior ring is always derived from the first set of coordinates in your object definition.</p>
<p>The POLYGON object in the WKT standard also has a <em>MULTIPOLYGON</em> counterpart for when you wish to define a multiple, <em>non intersecting</em> set of polygon objects which, in turn, can have as many interior rings as you like.</p>
<p>Other objects we have available in the WKT standard are:</p>
<ul>
<li>POINT(0 0) to represent a point on a grid</li>
<li>LINESTRING(0 0, 0 1) to represent a line. Note that a line can consist out of more then two coordinates.</li>
</ul>
<p>All of these also have a <em>MULTIPOINT</em> and a <em>MULTILINESTRING</em> variant respectively.</p>
<p>As we have seen before, all of these objects are two dimensional, but PostGIS also partly supports a three and a four dimensional version of some of these objects.</p>
<p><em>Note</em> that these extra dimensions are currently <em>not</em> in de specification and is a PostGIS specific extension on top of the features defined by the OGC.
Furthermore, if the OGC decides to standardize three of four dimensional objects, PostGIS will have to adapt its syntax to stay compliant.
We thus refer to this extended format not as WKT or WKB but as <em>Extended</em> WKT and <em>Extended</em> WKB or simply <em>EWKT</em> and <em>EWKB</em>.</p>
<p>To make our polygon object three dimensional, we could write it down like so:</p>
<div class="code"><pre><span class="n">POLYGON</span> <span class="p">((</span><span class="mi">4</span> <span class="mi">1</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span> <span class="mi">4</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="mi">4</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span> <span class="mi">1</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>


<p>You can see that we now have three numbers per coordinate, the third one adds a <em>Z</em> or <em>depth</em> value.</p>
<p>A point gets even more fancier. If we wish to place a point in three dimensional space, we could write it down the same as we did with our polygon:</p>
<div class="code"><pre><span class="n">POINT</span> <span class="p">(</span><span class="mi">4</span> <span class="mi">1</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>


<p>The third parameter here being, again, a place on the <em>Z</em> axis.</p>
<p>But points can also have a <em>fourth</em> dimension which sounds fancy, but is nothing more then an extra reference we can ship with our coordinates.
This reference, also called a <em>linear reference</em>, is a number we can put in place that tell us where, along a linear path, the point we define resides.</p>
<p>It can be written down like this:</p>
<div class="code"><pre><span class="n">POINT</span> <span class="p">(</span><span class="mi">4</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">6</span><span class="p">)</span>
</pre></div>


<p>Here we have four numbers, the last one being the linear reference or <em>M</em>.</p>
<p>With EWKT you also have the possibility to define a two dimensional object with a linear reference:</p>
<div class="code"><pre><span class="n">POINT</span> <span class="n">M</span><span class="p">(</span><span class="mi">4</span> <span class="mi">1</span> <span class="mi">6</span><span class="p">)</span>
</pre></div>


<p>Here we have again three numbers, but to distinguish between the last number being <em>Z</em> or <em>M</em>, we have to reference <em>M</em> together with our point declaration.
There are more extensions defined in the EWKT and EWKB, but that is slightly off-topic, because, as I mentioned before, these are not standardized.
In most use cases you can simply use the standard WKT and WKB forms.</p>
<h4>What to use these objects for?</h4>
<p>You now know what kind of objects we can represent using text and what we can, later along the road, insert into our PostGIS enabled PostgreSQL database.
But how do these points, lines and polygons help us measure distance or help us locate stuff?</p>
<p>First it is important to understand that all of the objects we have available will act as <em>proxies</em> to real world objects.
Take, for example, the point. A point can be used on a map to indicate a place, a spot so to speak, without defining shape or size.
When you wish to know where Tokyo is, a point will suffice on a global scale, you do not need nor want to know the exact shape of the metropolis.</p>
<p>However, if you would zoom in on our fictional map and you wish to see a part of the city the size of a few city blocks, you might be interested in the shapes of buildings, lakes, parks, etc.
These items that take up two <em>dimensional space</em> will be drawn with polygons that resemble the shape of the real world objects as close as possible.</p>
<p>Lines (or linestrings), finally, will almost always be used to represent roads, railroads, metro systems, etc. They many times represent actual <em>paths</em> one could travel along.</p>
<h3>Geometry and Geography</h3>
<p>So you know that you can represent a place in the world with a simple point.
And as you also know, a point is defined like this:</p>
<div class="code"><pre><span class="n">POINT</span> <span class="p">(</span><span class="mi">10</span> <span class="mi">20</span><span class="p">)</span>
</pre></div>


<p>This would create a point that sits at coordinate (10 20). But, what does that mean?
How do these numbers relate to the <em>real world</em>? What <em>is</em> 10 or 20 anyway?</p>
<p>Well, first you will have to ask yourself the following question: Do I wish to be Cartesian or Geographical?</p>
<h4>Cartesian or Geographical</h4>
<p>As you may or may not remember from your boring math lessons, a Cartesian system is a two dimensional flat grid with a X and a Y axis.
These axis go both positive and negative with the origin sitting exactly in the middle of the flat plane.</p>
<p>When working with GIS objects, we refer to this flat, Cartesian grid system as <em>Geometry</em>.
When, however we are working with measurements or objects related to the <em>real</em> earth we, in PostGIS, refer to these measurements as <em>Geography</em>.</p>
<p>Why? what is the difference? Well, to understand this, we have to take a step back, a step back into time that is.</p>
<p>When the first maps of the world where crafted, people truly believed the earth was flat (which it is not...for your information).
This meant that all charts that where drawn assumed we could simply place a grid comprised out of an X (length) and Y (height) axis across the drawing and from their measure distances between points. If you wish to know the distance between Paris and London, simply place two points on your map, take your <em>straight</em> ruler and measure the distance indicated.
Then factor in the chart's scale and you have your distance. You use <em>Geometry</em> or <em>Geometric measurements</em>.</p>
<p>However, after Copernicus nearly got his head chopped off telling people the earth was <em>not</em> flat, the chart drawing people gasped for air.
This meant their measuring technique was not correct. If the earth really was a sphere, then one could not simply wrap a grid around it and act as if everything was linear.
A sphere meant that there was a certain amount of distortion happening with their overlaying grid, and the measurements should encompass for those differences.</p>
<p>Even later in time, the chart drawing folk, who barely recovered from their first shock, where zapped again when people started to realize the earth was not a sphere either.
The globe turned out to be more of an egg shape, which, again, meant that measurement techniques had to be adjusted.</p>
<p>This was the birth of the <em>geographical</em> measurement system where cartographers devices a model called the <em>spheroid</em>.
A spheroid is a three dimensional object on which we can most accurately place points and measure real earth distances.
Each point on such a spheroid is define by a <em>latitude</em> and a <em>longitude</em>:</p>
<ul>
<li>Latitude is measured from the center of the earth (the hot place) in an angle up or down towards the surface</li>
<li>Longitude is measured from the same hot center in an angle left or right towards the surface</li>
</ul>
<p>Because both latitude and longitude represent an angle we express them as a <em>degrees</em> and we simply call the <em>geographical coordinates</em>.</p>
<p>Now, it is not quite convenient to have to carry around a three dimensional spheroid to find out where you are or to measure distance.
A classical old paper map is still more easy to bring along and more easy to work with.
But how do we go from a spheroid, which has the correct distortion, back to our old, flat, two dimensional geometrical map?</p>
<p>With <em>projection</em> or <em>map projection</em> to be more precise. We need to <em>project</em> the three dimensional spheroid system onto our two dimensional map.
This projecting is roughly done in three steps.</p>
<p>First we have to decide whether to take our spheroid as the base or a simpler sphere. A simpler sphere will yield less accurate results because it does not quite represent the correct curvature of the earth, but it does keep the maths behind the calculations simpler and thus can make for faster calculations. When choosing which shape we want, we also will have to define which <em>datum</em> we would like.</p>
<p>After choosing the base object and the datum that represents it, we have to transform the geographic system coordinates (latitude and longitude) to more standard X and Y coordinates to be used on a simple, flat, Cartesian plane. </p>
<p>The last part is to find out to what ratio the final two dimensional surface is scaled compared to the original, base object (which represents the earth).</p>
<h4>Datum?</h4>
<p>Before continuing, a word about datums.</p>
<p>As we said before, people agreed that the earth has a spheroid shape and that this model represents the earth most accurately.
We say "model" because the spheroid is something that is actually <em>defined</em> with math.</p>
<p>The math behind the spheroid model is what we call the <em>datum</em>. It is nothing more then a mathematical formula describing the shape.</p>
<p>Something we did not see is the fact that there actually are <em>many types</em> of spheroids out there. Each serving their own purpose and each with their own math aka datum.
Some spheroids are better to do measurements on a global scale, others are better for a more local "zoomed-in" level (continent, country, ...).</p>
<p>The reason we have to tell which datum (thus shape) our spheroid has, is because while latitude and longitude always represent degrees, they can have different meaning depending on the chosen datum.
If you use a datum that draws the spheroid a little bit "elongated" so to speak, then 1 degree longitude will cover slightly more distance then if the datum draws a more compact spheroid.</p>
<p>We will see more about datums in the next chapter, but it is an important part of GIS.</p>
<h3>Types of Projections</h3>
<p>Something that might not be as obvious right now is the fact that going from our three-dee globe to a flat surface is a process of choices.
In an ideal world you wish to keep every aspect of your spheroid intact, meaning the proportions of the objects on the map are accurate everywhere, the shape of these objects is correct, the area covered by the objects is true and the distance between these objects is retained.
However, as it turns out, this is impossible on a two dimensional surface. You have to give up some of these properties to preserve others.</p>
<p>Throughout history there have been many attempts at creating projections that would keep as much of these aspects intact.</p>
<h4>Mercator projection</h4>
<p>As a Belgian I should be most proud about this type of projection, since it was created by a fellow Flemish-man, around 450 years ago and it is a projection that is still being used today.
When a map is created with this type of projection we will get a comfortable and familiar view of the earth. 
A big advantage of this projection type is the fact that the shape of all objects are accurate.</p>
<p>The Mercator projection is most accurate around the equator, but the further you travel up or down, the more the map goes out of proportion.
Mercator used a cylindrical projection to unwrap the earth into a flat plane. Because of the nature of such a cylindrical projection, the areas more close to the poles become blown up to fit in a two dimensional world.</p>
<p>This distortion has caused quite some frowned foreheads in the last few decades and as a result people tend to abandon this projection, specially to project regions far from the equator.</p>
<h4>Mercator variants</h4>
<p>To make up for the heavy distortions found in the original Mercator system, people have made two new Mercator projections.
The first that came about was called the <em>Transverse Mercator</em> which fixes the distortions around the poles, but introduces the problem that it will make for incorrect distance measuring.</p>
<p>To make up for this new problem, folks made yet another Mercator derivative: the <em>Universal Transverse Mercator</em>. This type of projection takes a whole new approach and uses its own coordinate system.
It introduces the concept of UTM zones. The earth is divided into roughly 60 zones and are each about 800 Km wide. The map that is rendered in each single zone uses the previous, Transverse Mercator projection to draw the actual map. A big advantage of this approach is the fact that we get a very constant distance measurement all across the globe.</p>
<p>Such a UTM coordinate looks quite different from our classic latitude/longitude or our X/Y version. I will give you a random UTM coordinate:</p>
<div class="code"><pre><span class="mi">54</span> <span class="n">N</span> <span class="mi">384524</span><span class="p">.</span><span class="mi">5</span> <span class="mi">3948304</span><span class="p">.</span><span class="mi">4</span>
</pre></div>


<p>The first number identifies one of the 60 UTM zones. The letter N show us in which hemisphere we should search this zone. These letters range from C to X (omitting I and O).
The first float tell us the <em>easting</em>, or X value, the last float tells us the <em>northing</em> or the Y value. Both these floats represent actual meters. </p>
<p>Another important note to take about UTM is that it also acts as a framework for more localized UTM versions.
This means that each country or region could make its own maps, using smaller UTM zones to accurately represent their land, city, forest, etc.</p>
<h4>Lambert Azimuthal</h4>
<p>This projection (also called the Lambert Equal-Area) is yet another approach as it uses a <em>disc</em> to map our spheroid to a flat surface.</p>
<p>The big advantage of this type of projection is the fact that it represent the area of objects very accurately and is true regarding distance calculation.
However, it fails when it comes to accurate shape representation for shapes get more and more distorted once you start moving away from the center of the disc.</p>
<p>The Lambert projection is one of the more accepted projections, right after the UTM.</p>
<h4>Plate CarrÃ©e</h4>
<p>And then you have Plate CarrÃ©e.</p>
<p>This is one of the oldest projections out there and was invented around 1800 years ago.
In our little history story above, this projection came about when people thought the earth was rather flat.</p>
<p>It combines almost all disadvantages of previous projections:</p>
<ul>
<li>It does a terrible job in representing correct area</li>
<li>It does not care about the shape of objects</li>
<li>Distance measuring is way off</li>
</ul>
<p>Despite the fact that this projection turns out to be so terrible, it is still quite commonly used today.
Well, not for navigation or distance calculation, obviously, but for illustrative purposes.</p>
<p>Many organizations across the globe use this simple projection to demonstrate statistical data, overlaid on this map.
Demographics, political info, zombie outbreak danger zones, ... .</p>
<p>As we also saw in our history lesson, the first charts used the Cartesian system quite literally and without much conversion, because the earth was flat anyway.
So in GIS systems, this means that this projection maps latitude and longitude <em>directly</em> to a X and Y coordinate without much conversion.</p>
<p>Because the conversion math is simple and calculations are few, this projection is among the fastest, but as you know now, at great cost.</p>
<h4>Other variants</h4>
<p>There are numerous other variants out there that all have their advantages or disadvantages. To name a few more:</p>
<ul>
<li>The Robinson projection displays the earth not in a flat image, but in a cylindrical flat sphere. It show the world more accurately, but it fails when it comes to representing area and shape, especially near the poles.</li>
<li>The Winkel Tripel projection is another popular projection type which has many parallels with the Robison one, but has less distortion.</li>
<li>The Peirce quincuncial projection uses a technique to unwrap the earth spheroid into a square, much like you would peel an orange. These maps are not used much, for they are very heavy in calculations, but the technique is now widely used to present a spherical image, unwrapped into a square.</li>
<li>The Goode homolosine projection is a projection developed as a teaching instrument in a frustrating answer to the heavily distorted Mercator projection. It is famous for its quite unique shape where the spheroid is unwrapped into a beast with four "legs".</li>
<li>...</li>
</ul>
<h3>What do projections mean to us?</h3>
<p>There are many more types of projections, but that would bore you to tears.</p>
<p>The fact that I keep going on about projections, geometry and geography is because later on, when working with PostGIS, you will need to make a decision about how you wish to combine all of these.</p>
<p>First it is very important to understand that geometry and geography are <em>two different data types</em> which PostGIS can store into PostgreSQL.</p>
<p>PostGIS is quite unique in the fact that it gives you the ability to work <em>directly</em> with our three dimensional spheroid (geography) and ignore the projections and their Cartesian Flat Land (geometry).
You will have the power to work with latitude and longitude and perform real world calculations, right out of the box.
This way of working, however, comes with a few trade-offs.</p>
<p>The first, and most obvious one: real, three dimensional spheroid geographical calculations will cost more computing time then the simpler, two dimensional geometry counterparts.
Another disadvantage of geography over geometry is the fact that PostGIS simply has <em>much</em> less native functions ready for you to use.</p>
<p>So depending on your use case, it might be a good idea to convert all your geographical data into geometrical ones.
This, however, requires knowledge about the projections we just saw for different projections will yield different results.</p>
<p>If you have two points with a latitude and longitude coordinate (thus being geographical data) and wish to know the distance between them using geometrical functions, you have to project these points on a flat surface thus converting them into a Cartesian system (the whole projection story we saw so far). </p>
<p>As we will see in the next chapter, if you simply convert geography into geometry, PostGIS will project the geometry coordinates using the Plate CarrÃ©e, which may not be very desirable when you which to calculate distances as we will be doing later on. We have the ability to tell PostGIS to use a different projection when converting, but all come with merits and demerits.</p>
<p>You simply cannot do serious GIS work if you do not have at least a basic understanding of what is going on when projecting geography.
By reading through this chapter, I hope I have given you enough food-for-thought to go out and explore a bit more about these different projections.</p>
<h3>What is next?</h3>
<p>Okay, I think we have covered enough for today. I do apologize for the rather theoretical nature of this first chapter, but believe me, you will need the knowledge.</p>
<p>Next time we will finally be looking at some actually PostGIS work and put some of this theory into practice:</p>
<ul>
<li>We will see how to GIS enable your PostgreSQL database</li>
<li>We will look at how we can store geometry and geography</li>
<li>We will actually put some points on the earth, draw some lines between them and perform some fun calculations</li>
<li>We will take a look at how different projections will yield different results</li>
<li>And finally, we will answer the question that started it all: How far is Tokyo from your current location?</li>
</ul>
<p>And as always...thanks for reading!</p>
<!--  LocalWords:  PostGIS GIS PostgreSQL
 -->
    </div>
    <div class="feedEntry">

        <a href="http://thebuild.com/blog/2014/06/11/the-djangocon-us-2014-call-for-proposals-is-open/" title="Christophe Pettus: The Djangocon US 2014 Call for Proposals is open!">
            <h2>Christophe Pettus: The Djangocon US 2014 Call for Proposals is open!</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 11, 2014</span>.
            
        </p>

        <p>As program chair this year, I am very pleased to announce that the <a href="http://www.djangocon.us/call_for_proposals/">Call for Proposals</a> for Djangocon US 2014 is now open! It only runs through the end of June, so be sure to get them in promptly!</p>
    </div>
    <div class="feedEntry">

        <a href="http://bonesmoses.org/2014/06/11/foreign-tables-are-not-as-useful-as-i-hoped/" title="Shaun M. Thomas: Foreign Tables are not as Useful as I Hoped">
            <h2>Shaun M. Thomas: Foreign Tables are not as Useful as I Hoped</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 11, 2014</span>.
            
        </p>

        <p>When I heard about foreign tables using the new <code>postgres_fdw</code> foreign data wrapper in PostgreSQL 9.3, I was pretty excited. We hadn&#8217;t upgraded to 9.3 so I waited until we did before I did any serious testing. Having done more experimentation with it, I have to say I&#8217;m somewhat disappointed. Why? Because of how authentication was implemented.</p>

<p>I&#8217;m going to get this out of the way now: The <code>postgres_fdw</code> foreign data wrapper only works with hard-coded plain-text passwords, forever the bane of security-conscious IT teams everywhere. These passwords aren&#8217;t even obfuscated or encrypted locally. The only implemented security is that the <code>pg_user_mapping</code> table is limited to superuser access to actually see the raw passwords. Everyone else sees this:</p>

<pre><code>postgres=&gt; SELECT * FROM pg_user_mapping;
ERROR:  permission denied for relation pg_user_mapping
</code></pre>

<p>The presumption is that a database superuser can change everyone&#8217;s password anyway, so it probably doesn&#8217;t matter that it&#8217;s hardcoded and visible in this view. And the developers have a point; without the raw password, how can a server-launched client log into the remote database? Perhaps the real problem is that there&#8217;s no mechanism for forwarding authentication from database to database.</p>

<p>This is especially problematic when attempting to federate a large database cluster. If I have a dozen nodes that all have the same user credentials, I have to create mappings to every single user, for every single foreign table, on every single independent node, or revert to trust-based authentication.</p>

<p>This can be scripted to a certain extent, but to what end? If a user were to change their own password, this breaks every foreign data wrapper they could previously access. This user now has to give their password to the DBA to broadcast across all the nodes with modifications to the user mappings. In cases where LDAP, Kerberos, GSSAPI, peer, or other token forwarding authentication is in place, this might not even be possible or advised.</p>

<p>Oracle solved this problem by tying DBLINK tables to a specific user during creation time. An access to a certain table authenticates as that user in all cases. This means a DBA can set aside a specific user for foreign table access purposes, and use a password that&#8217;s easy to change across the cluster if necessary. Grants take care of who has access to these objects. Of course, since <code>postgres_fdw</code> is read/write, this would cause numerous permissions concerns.</p>

<p>So what are we left with? How can we actually use PostgreSQL foreign tables securely? At this point, I don&#8217;t believe it&#8217;s possible unless I&#8217;m missing something. And I&#8217;m extremely confused at how this feature got so far along without any real way to lock it down in the face of malleable passwords. Our systems have dozens of users who are forced by company policy to change their passwords every 90 days, thus none of these users can effectively access any foreign table I&#8217;d like to create.</p>

<p>And no, you can&#8217;t create a mapping and then grant access to it. In the face of multiple mapping grants, which one would PostgreSQL use? No, if there&#8217;s a way to solve this particular little snag, it won&#8217;t be that convenient. If anyone has ideas, or would like to go into length at how wrong I am, please do! Otherwise, I&#8217;m going to have to use internal users of my own design and materialized views to wrap the foreign tables; extremely large tables will need some other solution.</p>
    </div>
    <div class="feedEntry">

        <a href="http://www.cybertec.at/reducing-log-messages-on-the-client/" title="Hans-Juergen Schoenig: Reducing log messages on the client">
            <h2>Hans-Juergen Schoenig: Reducing log messages on the client</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 11, 2014</span>.
            
        </p>

        It happens quite frequently that PostgreSQL client applications are flooded with messages from the server. This is both annoying and bad for performance as well as network bandwidth. It seems that many users are not aware of the fact that this flood of log messages can easily be reduced by simply changing the server configuration. [&#8230;]
    </div>
    <div class="feedEntry">

        <a href="http://rhaas.blogspot.com/2014/06/linux-disables-vmzonereclaimmode-by.html" title="Robert Haas: Linux disables vm.zone_reclaim_mode by default">
            <h2>Robert Haas: Linux disables vm.zone_reclaim_mode by default</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 10, 2014</span>.
            
        </p>

        Last week, Linus Torvalds merged a Linux kernel commit from Mel Gorman <a href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=4f9b16a64753d0bb607454347036dc997fd03b82">disabling vm.zone_reclaim_mode by default</a>.&nbsp;&nbsp; I mentioned that this change might be in the works when <a href="http://rhaas.blogspot.com/2014/03/back-from-lsfmm-and-collab.html">I blogged about attending LSF/MM</a> and again when I blogged about how <a href="http://rhaas.blogspot.com/2014/04/subtly-bad-things-linux-may-be-doing-to.html">the page cache may not behave quite the way we want even with vm.zone_reclaim_mode disabled</a>.<br /><br />For those who haven't read previous discussion on this topic, either on my blog, on pgsql-performance, or elsewhere around the Internet, enabling vm.zone_reclaim_mode can cause a lot of problems for applications, such as <a href="http://www.postgresql.org/">PostgreSQL</a>, that make use of more page cache than will fit on a single NUMA node.&nbsp; Pages may get evicted from memory in preference to using memory on other nodes, effectively resulting in a page cache that is much smaller than available free memory.&nbsp; See the second of the two blog posts linked above for more details.<br /><br />PostgreSQL isn't the only application that suffers from non-zero values of this setting, so I think a lot of people will be happy to see this change merged (like the guy who said that <a href="http://www.poempelfox.de/blog/2010/03/19/">this setting is the essence of all evil</a>).&nbsp; It will doubtless take some time for this to make its way into mainstream Linux distributions, but getting the upstream change made is the first step.&nbsp; Thanks to Mel Gorman for pursuing this.
    </div>
    <div class="feedEntry">

        <a href="http://pdxpug.wordpress.com/2014/06/09/pdxpug-june-lab-night-this-week/" title="gabrielle roth: PDXPUG June Lab Night this week">
            <h2>gabrielle roth: PDXPUG June Lab Night this week</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 09, 2014</span>.
            
        </p>

        <p>For this month&#8217;s lab, we&#8217;ll work our way through the <a href="https://schemaverse.com/" target="_blank">Schemaverse</a> tutorial.  Bring your laptop and your SQL skills!  Dinner is BYO this month, but beverages will be provided.</p>
<p>Space is limited, so please <a href="https://docs.google.com/forms/d/1XPtT2wVQnA35iSvUUMYGB-4MDpfGSJctcmLPVZzqqUI/viewform?usp=send_form" target="_blank">sign up in advance</a>.</p><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/pdxpug.wordpress.com/366/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/pdxpug.wordpress.com/366/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=pdxpug.wordpress.com&#038;blog=30930172&#038;post=366&#038;subd=pdxpug&#038;ref=&#038;feed=1" width="1" />
    </div>
    <div class="feedEntry">

        <a href="http://adpgtech.blogspot.com/2014/06/buildfarm-vs-vpath-vs-ccache.html" title="Andrew Dunstan: buildfarm vs vpath vs ccache">
            <h2>Andrew Dunstan: buildfarm vs vpath vs ccache</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 08, 2014</span>.
            
        </p>

        I think we've got more or less to the bottom of the ccache mystery I wrote about recently. It turns out that the problem of close to 100% of cache misses occurs only when the buildfarm is doing a vpath build, and then only because the buildfarm script sets up a build directory that is different each run ("pgsql.$$"). There is actually no need for this. The locking code makes sure that we can't collide with ourselves, so a hardcoded name would do just as well. This was simple an easy choice I made, I suspect without much thought, 10 years ago or so, before the buildfarm even supported vpath builds.<br /><br />It also turns out there is no great point in keeping a separate cache per branch. That was a bit of a thinko on my part.<br /><br />So, in my lab machine ("crake") I have made these changes: the build directory is hard coded with a ".build" suffix rather than using the PID. And it keeps a single cache, not one per branch. After making these changes, warming the new cache, and zeroing the stats, I did fresh builds on each branch. Here's what the stats looked like (cache compression is turned on):<br /><blockquote><pre>cache directory                     ccache<br />cache hit (direct)                  5988<br />cache hit (preprocessed)             132<br />cache miss                             0<br />called for link                     1007<br />called for preprocessing             316<br />compile failed                       185<br />preprocessor error                    69<br />bad compiler arguments                 6<br />autoconf compile/link                737<br />no input file                         25<br />files in cache                     12201<br />cache size                         179.8 Mbytes<br />max cache size                       1.0 Gbytes<br /></pre></blockquote><br />So I will probably limit this cache to, say, 300MB or so. That will be a whole lot better than the gigabytes I was using previously.<br /><br />As for the benefits: on HEAD "make -j 4" now runs in 13 seconds on crake, as opposed to 90 seconds or more previously.<br /><br />If we have a unified cache, it makes sense to disable the removal of the cache in failure cases, which is what started me looking at all this. We will just need to be a bit vigilant about failures, as many years ago there was at least some suspicion of persistent failures due to ccache problems.<br /><br />All this should be coming to a buildfarm release soon, after I have let this test setup run for a week or so.<br />
    </div>
    <div class="feedEntry">

        <a href="http://www.depesz.com/2014/06/08/anonymize-cte-names-on-explain-depesz-com/" title="Hubert 'depesz' Lubaczewski: Anonymize CTE names on explain.depesz.com">
            <h2>Hubert 'depesz' Lubaczewski: Anonymize CTE names on explain.depesz.com</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 07, 2014</span>.
            
        </p>

        A colleague recently let me know that anonymization in explain.depesz.com doesn't handle CTE names. For example, in plan: QUERY PLAN --------------------------------------------------------------------------------------------------- CTE Scan ON some_name &#40;cost=0.01..0.03 ROWS=1 width=8&#41; &#40;actual TIME=0.027..0.028 ROWS=1 loops=1&#41; CTE some_name -&#62; RESULT &#40;cost=0.00..0.01 ROWS=1 width=0&#41; &#40;actual TIME=0.023..0.023 ROWS=1 loops=1&#41; Planning TIME: 0.217 ms Execution TIME: 0.124 ms &#40;5 ROWS&#41; &#8220;some_name" was [&#8230;]
    </div>
    <div class="feedEntry">

        <a href="http://gorthx.wordpress.com/2014/06/06/autovacuum-long-naps-arent-better/" title="gabrielle roth: autovacuum:  long naps arenât better">
            <h2>gabrielle roth: autovacuum:  long naps arenât better</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 06, 2014</span>.
            
        </p>

        There&#8217;s that saying about &#8220;the first time&#8217;s an accident, the second&#8217;s a coincidence, the third is a pattern&#8221;. It&#8217;s probably because I&#8217;ve been studying Postgres&#8217;s autovacuum feature so much lately and these things stand out to me now, but I&#8217;ve noticed a really intriguing pattern (n&#62;5) over the past month or so: folks with their [&#8230;]<img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=gorthx.wordpress.com&#038;blog=32848600&#038;post=920&#038;subd=gorthx&#038;ref=&#038;feed=1" width="1" />
    </div>
    <div class="feedEntry">

        <a href="http://adpgtech.blogspot.com/2014/06/ccache-mysteries.html" title="Andrew Dunstan: ccache mysteries">
            <h2>Andrew Dunstan: ccache mysteries</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 06, 2014</span>.
            
        </p>

        <a href="http://ccache.samba.org/">ccache</a> is a nifty little utility for speeding up compilations by caching results. It's something we have long had support for in the buildfarm. <br /><br />Tom Lane pinged me a couple of days ago about why, when there's a build failure, we remove the ccache. The answer is that a long time ago (about 8 years), we had some failures that weren't completely explained but where suspicion arose that ccache was returning stale compilations when it shouldn't have been. I didn't have a smoking gun then, and I certainly don't have one now. Eight years ago we just used this rather elephant-gun approach and moved on.<br /><br />But Now I've started looking at the whole use of ccache. And the thing I find most surprising is that the hit rate is so low. Here, for example, are the stats from my FreeBSD animal nightjar, after a week since a failure:<br /><br /><blockquote><pre>cache directory                     HEAD<br />cache hit (direct)                  2540<br />cache hit (preprocessed)              45<br />cache miss                         32781<br />called for link                     5571<br />called for preprocessing            1532<br />compile failed                       899<br />preprocessor error                   248<br />bad compiler arguments                31<br />autoconf compile/link               3990<br />no input file                        155<br />files in cache                     25114<br />cache size                         940.9 Mbytes<br />max cache size                       1.0 Gbytes<br /></pre></blockquote>So I'm a bit puzzled. Most changes that trigger a build leave most of the files intact. Surely we should have a higher hit rate than 7.3%. If that's the best we can do It seems like there is little value in using ccache for the buildfarm. If it's not the best we can do I need to find out what I need to change to get that best. But so far nothing stands out.<br /><br />Tom also complained that we keep a separate cache per branch. The original theory was that we would be trading disk space for a higher hit rate, but that seems less tenable now, with some hindsight.
    </div>
    <div class="feedEntry">

        <a href="http://okbob.blogspot.com/2014/06/new-postgresql-94-article.html" title="Pavel Stehule: New PostgreSQL 9.4 article">
            <h2>Pavel Stehule: New PostgreSQL 9.4 article</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 06, 2014</span>.
            
        </p>

        I upgraded a mediawiki 1.23 on our Czech PostgreSQL site. New important feature is a integration of Google Translator. Now a articles from this site can be read in seven languages. New one is my fresh article about <a href="http://postgres.cz/wiki/PostgreSQL_9.4_%282014%29:_transak%C4%8Dn%C3%AD_sql_json_datab%C3%A1ze">PostgreSQL 9.4 new features</a>
    </div>
    <div class="feedEntry">

        <a href="http://pdxpug.wordpress.com/2014/06/05/pdxpug-june-meeting-in-two-weeks-2/" title="gabrielle roth: PDXPUG:  June meeting in two weeks">
            <h2>gabrielle roth: PDXPUG:  June meeting in two weeks</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 05, 2014</span>.
            
        </p>

        <p><strong>When</strong>: 7-9pm Thu June 19, 2014<br />
<strong>Where</strong>: Iovation<br />
<strong>Who</strong>: David Kerr<br />
<strong>What</strong>: Postgres on Docker</p>
<p>David Kerr&#8217;s back this month to talk about Postgres on Docker.</p>
<p>Linux containers are finally getting their day, and leading the way is a project known as Docker. Docker is in use today at places like Google, eBay, and New Relic and aims to change the way we think about deploying software.</p>
<p>Join us as we get a quick primer on Docker and then run it through its paces with the not-so-typical, but interesting, use-case of running PostgreSQL in a container.</p>
<p>We&#8217;ll look at benchmarks, and various novel use cases where Docker may be an interesting fit in the database world.</p>
<p>Dave Kerr is a recovering DBA, PostgreSQL evangelist and is currently working as a Software Engineer on the Site Engineering team at New Relic.</p>
<p>&#8211;</p>
<p>We have a new meeting location while the Iovation offices are being renovated.  We&#8217;re still in the US Bancorp Tower at 111 SW 5th (5th &amp; Oak), but on the ground floor in the Training Room.  As you face the bank of elevators from the main lobby, take a good deep breath of drywall dust and head to your right to a hallway.  Follow the hallway as it turns to the left.  The Training Room is the 2nd door on your right and is labeled &#8220;Training Room&#8221;.  There is no room number.</p>
<p>The building is on the Green &amp; Yellow Max lines.  Underground bike parking is available in the parking garage;  outdoors all around the block in the usual spots.</p>
<p>See you there!</p><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/pdxpug.wordpress.com/363/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/pdxpug.wordpress.com/363/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=pdxpug.wordpress.com&#038;blog=30930172&#038;post=363&#038;subd=pdxpug&#038;ref=&#038;feed=1" width="1" />
    </div>
    <div class="feedEntry">

        <a href="http://raghavt.blogspot.com/2014/06/utilising-caching-contribs-pgprewarm.html" title="Raghavendra Rao: Utilising caching contrib's pg_prewarm and pg_hibernator in PostgreSQL 9.4.">
            <h2>Raghavendra Rao: Utilising caching contrib's pg_prewarm and pg_hibernator in PostgreSQL 9.4.</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 05, 2014</span>.
            
        </p>

        <div dir="ltr" style="text-align: left;">Numerous DBA's (counting me), put questions all the time to PostgreSQL hackers/developers/architects on mailing list:<br /><div style="text-align: left;"></div><ul style="text-align: left;"><li>Q1. Does PG has the ability to cache/warm a relation ?</li><li>Q2. Is it possible to return to prior state of cache where it was left before shutting down the database server because of maintenance ?</li></ul><br />In earlier releases of PostgreSQL, there in no chance of warming a relation or storing a cache states, but from PostgreSQL 9.4 onwards each of the above queries(Q1,Q2) addressed with two contrib modules <i>pg_prewarm</i> and <i>pg_hibernator</i>. Despite the very fact that they're distinctive  in practicality, however the combination appears to be extremely viable and useful in future for DBA's. In short about contrib's:<br /><blockquote><i><b><a href="http://www.postgresql.org/docs/9.4/static/pgprewarm.html" target="_blank">pg_prewarm </a></b></i>contrib (Author: <a href="http://rhaas.blogspot.com/" target="_blank">Robert Haas</a>), provides the capability to load a relation data into OS buffer cache or PG buffer cache. It has the functionality of first or last block number to prewarm. (Note: It has no special protection on pre-warmed data from cache eviction and also if database instance restarted then re-warm needed on the relations).</blockquote><blockquote><b><i><a href="https://github.com/gurjeet/pg_hibernator" target="_blank">pg_hibernator </a></i></b>contrib (Author: <a href="http://gurjeet.singh.im/" target="_blank">Gurjeet Singh</a>), provides the capability to automatically save the list of shared buffer contents to disk on database shutdown, and automatically restores the buffers on database startup, much the same as save/restore a snapshot of shared_buffers. It make use PG 9.3 module to register "background worker process" and spawns two process "Buffer Saver", "Buffer Reader" for save/restore. Interestingly, with a little hack, pg_hibernator can also allow standby slave to start serving queries with full speed with same contents of master, will see that in a minute :).</blockquote>Lastly, we need<i><b><a href="http://www.postgresql.org/docs/9.4/static/pgbuffercache.html" target="_blank"> pg_buffercache</a></b></i> module to look inside the current contents of PostgreSQL shared_buffers. This module helps to understand what percentage buffer's occupied by a relation.<br /><br />Let's put all these contrib's into play and see how they serve the purpose of two questions(Q1,Q2). Am going to use a table 'foo' of size 885MB on my local VM, along with a standard pg_buffercache query.<br /><pre class="cpp" name="code">SELECT c.relname,<br />       count(*) AS buffers<br />FROM pg_class c<br />INNER JOIN pg_buffercache b ON b.relfilenode=c.relfilenode AND c.relname='foo'<br />INNER JOIN pg_database d ON (b.reldatabase=d.oid AND d.datname=current_database())<br />GROUP BY c.relname<br />ORDER BY 2 DESC LIMIT 10;<br /></pre>Usage of pg_prewarm contrib and warming 'foo' table.<br /><pre class="cpp" name="code">postgres=# create extension pg_prewarm;<br />CREATE EXTENSION <br />postgres=# \dt+<br />                    List of relations<br /> Schema | Name | Type  |  Owner   |  Size  | Description<br />--------+------+-------+----------+--------+-------------<br /> public | foo  | table | postgres | 885 MB |<br />(1 row)<br />postgres=# select pg_prewarm('foo');<br /> pg_prewarm<br />------------<br />     113278<br />(1 row)<br />--pg_buffercache query output<br /> relname | buffers<br />---------+---------<br /> foo     |  113278<br />(1 row)<br /></pre>Very simple and straightforward usage of <i>pg_prewarm</i> with a output of blocks warmed in shared_buffers for relation 'foo'. From <i>pg_buffercache</i> query output we can evaluate it that there are <span style="color: blue;">113278 (113278 * 8 / 1024 = 884MB ) </span>buffers of 8KB block size of relation 'foo' which matches with pg_prewarm output. Here, if Postgres server restarts because of some reason, shared_buffers are empty and DBA's need to re-warm again to come back to past warm stage. For a single table, re-warm is always simple except for a group of tables its agony.<br /><br />At this point, we can make use of pg_hibernator contrib, because it has the flexibility to save the shared_buffer's contents and restore it back at startup. Let's enable pg_hibernator/pg_prewarm together and run a similar exercise by simply including one step of restart and see if the cache state return back as is or not. Am not going to cover <a href="https://github.com/gurjeet/pg_hibernator" target="_blank">installation of pg_hibernator</a>, because on git its very well described, however I would directly jump to implementation part and start the server with pg_hibernator.<br /><pre class="cpp" name="code">postgres 24623     1  0 02:06 pts/4    00:00:00 /usr/local/pgpatch/pg/bin/postgres -D /usr/local/pgpatch/pg/data_10407<br />postgres 24627 24623  0 02:06 ?        00:00:00 postgres: logger process<br />postgres 24631 24623  0 02:06 ?        00:00:00 postgres: checkpointer process<br />postgres 24632 24623  0 02:06 ?        00:00:00 postgres: writer process<br />postgres 24633 24623  0 02:06 ?        00:00:00 postgres: wal writer process<br />postgres 24634 24623  0 02:06 ?        00:00:00 postgres: autovacuum launcher process<br />postgres 24635 24623  0 02:06 ?        00:00:00 postgres: archiver process<br />postgres 24636 24623  0 02:06 ?        00:00:00 postgres: stats collector process<br />postgres 24637 24623  0 02:06 ?        00:00:00 postgres: bgworker: Buffer Saver<br />postgres 24638 24623 11 02:06 ?        00:00:01 postgres: bgworker: Block Reader 2<br /><br />In database server logs at startup time:<br /><br />-bash-4.1$ more postgresql-2014-06-02_083033.log<br />LOG:  database system was shut down at 2014-06-02 08:13:00 PDT<br />LOG:  starting background worker process "Buffer Saver"<br />LOG:  database system is ready to accept connections<br />LOG:  autovacuum launcher started<br /></pre>Since, its first time pg_hibernator in play, you can see two process and also logs with some information regarding start of "Buffer Saver". Now, lets prewarm relation 'foo' and restart the server, later check the buffer status whether pg_hibernator filled the buffer back where it was left.<br /><pre class="cpp" name="code">-bash-4.1$ psql -p 10407<br />psql (9.4beta1)<br />Type "help" for help.<br /><br />postgres=# select pg_prewarm('foo');<br /> pg_prewarm<br />------------<br />     113278<br />(1 row)<br /><br />--pg_buffercache query output<br /> relname | buffers<br />---------+---------<br /> foo     |  113278<br />(1 row)<br />postgres=# \q<br /><br />-bash-4.1$ /usr/local/pgpatch/pg/bin/pg_ctl -D /usr/local/pgpatch/pg/data_10407 stop<br />waiting for server to shut down.... done<br />server stopped<br /><br />-bash-4.1$ ls -l $PGDATA/pg_hibernator/<br />total 12<br />-rw------- 1 postgres postgres  160 Jun  3 01:41 1.global.save<br />-rw------- 1 postgres postgres  915 Jun  3 01:41 2.postgres.save  <br /><br />-bash-4.1$ /usr/local/pgpatch/pg/bin/pg_ctl -D /usr/local/pgpatch/pg/data_10407 start<br />server starting<br /></pre>We have restarted the database server, lets examine the logs <br /><pre class="cpp" name="code">-bash-4.1$ more postgresql-2014-06-03_020601.log<br />LOG:  database system was shut down at 2014-06-03 02:05:57 PDT<br />LOG:  starting background worker process "Buffer Saver"<br />LOG:  database system is ready to accept connections<br />LOG:  autovacuum launcher started<br />LOG:  registering background worker "Block Reader 2"<br />LOG:  starting background worker process "Block Reader 2"<br />LOG:  Block Reader 2: restored 113433 blocks<br />LOG:  Block Reader 2: all blocks read successfully<br />LOG:  worker process: Block Reader 2 (PID 24638) exited with exit code 1<br />LOG:  unregistering background worker "Block Reader 2"<br />LOG:  registering background worker "Block Reader 1"<br />LOG:  starting background worker process "Block Reader 1"<br />LOG:  Block Reader 1: restored 20 blocks<br />LOG:  Block Reader 1: all blocks read successfully<br />LOG:  worker process: Block Reader 1 (PID 24664) exited with exit code 1<br />LOG:  unregistering background worker "Block Reader 1"<br /></pre>So, "Buffer Reader" has restored blocks of <span style="color: blue;">113433 + 20,</span> out of which <span style="color: blue;">113278</span> belongs to relation 'foo'. Great, lets connect and see.<br /><pre class="cpp" name="code">-bash-4.1$ psql -p 10407<br />psql (9.4beta1)<br />Type "help" for help.<br /><br />--pg_buffercache query output<br /> relname | buffers<br />---------+---------<br /> foo     |  113278<br />(1 row)<br /></pre>Cool... pg_hibernator has brought back the cache warmed state without DBA's interference. <br /><br />Another good thing about pg_hibernator, a newly created standby can have the same shared buffer contents as the master, so that the standby can start serving queries at full speed. To do this exercise, while taking a backup of $PGDATA directory, I have passed <span style="color: blue;">SIGTERM</span> to "Buffer Saver" process so that it writes the current state shared_buffers content to disk($PGDATA/pg_hibernator directory) and then followed with standby setup. <br /><pre class="cpp" name="code">postgres 24637 24623  0 02:06 ?        00:00:00 postgres: bgworker: Buffer Saver<br />postgres 24653 15179  0 02:06 ?        00:00:01 postgres: wal receiver process   streaming 1/6A000A10<br />postgres 24654 24623  0 02:06 ?        00:00:00 postgres: wal sender process postgres ::1(65011) streaming 1/6A000A10<br /></pre>After setup, my slave started with same content of primary<br /><pre class="cpp" name="code">-bash-4.1$ psql -p 10477<br />psql (9.4beta1)<br />Type "help" for help.<br /><br />postgres=# select pg_is_in_recovery();<br /> pg_is_in_recovery<br />-------------------<br /> t<br />(1 row)<br /><br />--pg_buffercache query output<br /> relname | buffers<br />---------+---------<br /> foo     |  113278<br />(1 row)<br /></pre>Thanks to both the authors for a wonderful extension on caching.<br /><br />--Raghav</div>
    </div>
    <div class="feedEntry">

        <a href="http://michael.otacoo.com/postgresql-2/logical-replication-chain-reaction/" title="Michael Paquier: Chain reaction with logical replication">
            <h2>Michael Paquier: Chain reaction with logical replication</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 05, 2014</span>.
            
        </p>

        <p>Working with logical replication and a cluster of nodes heavily linked
among each other can result in particularly disturbing results. If a
logical change is received on a node that itself regenerates additional
changes, it is really easy to finish with an uncontrolable chain reaction.</p>

<p>For example, here is an example with two nodes, running locally and
listening to ports 5432 and 5433. Both nodes are using the background
worker <a href="https://github.com/michaelpq/pg_plugins/tree/master/receiver_raw">receiver_raw</a> able
to receive changes generated by <a href="https://github.com/michaelpq/pg_plugins/tree/master/decoder_raw">decoder_raw</a> on a
given replication slot.</p>

<p>Both nodes have many settings in common in postgresql.conf:</p>
<div class="highlight"><pre><code class="text language-text">wal_level = logical
shared_preload_libraries = 'receiver_raw'
max_replication_slots = 1
log_line_prefix = '%t'
receiver_raw.database = 'postgres'
receiver_raw.slot_name = 'slot'
</code></pre></div>
<p>receiver_raw.database refers to the database on which changes received
from remote are applied locally, aka the database the background worker
connect to. receiver_raw.slot_name is the name of the slot from which
the logical changes are decoded and fetched.</p>

<p>However there is a subtile difference for receiver_raw.conn_string,
which is the connection string used by the background worker to connect
to the node generating the changes. Here it is for the first node:</p>
<div class="highlight"><pre><code class="text language-text">receiver_raw.conn_string = 'replication=database dbname=postgres port=5433'
</code></pre></div>
<p>And for the second node:</p>
<div class="highlight"><pre><code class="text language-text">receiver_raw.conn_string = 'replication=database dbname=postgres port=5432'
</code></pre></div>
<p>With this configuration each node connects to each other... By default
receiver_raw naps 100ms after a process loop (processing a single batch
of changes). And of course both nodes need to have a replication slot
created with name &quot;slot&quot;. Let's use at the same time a simple schema.</p>
<div class="highlight"><pre><code class="text language-text">=# SELECT slotname FROM pg_create_logical_replication_slot('slot', 'decoder_raw');
 slotname 
----------
 slot
(1 row)
=# CREATE TABLE aa (a int);
CREATE TABLE
</code></pre></div>
<p>Then using \watch with a psql client, you can see the evolution of the
number of records on this table, after running on it a simple INSERT
query.</p>
<div class="highlight"><pre><code class="text language-text">=# INSERT INTO aa VALUES (1);
INSERT 0 1
=# SELECT count(*) FROM aa;
 count 
-------
     2
(1 row)
=# \watch 1
Watch every 1s  Thu Jun  5 22:24:47 2014
 count 
-------
     8
(1 row)
[... etc ...]
</code></pre></div>
<p>With an average of 6-8 INSERT queries run per second on a single node,
this can blow up your disk quickly, because of the queries of course
getting repeated, but as well because of the replication slots that
need to keep more and more data as process loop sleeps for a
customized amount of time. This gets even worse if more and more new
queries are run. So the moral of the story is simple: don't do that!
Then, control the origin of the data to stop the hemoragy created by
continuously-applied changes in loop.</p>
    </div>
    <div class="feedEntry">

        <a href="http://danielpocock.com/trialing-the-xtuple-postbooks-next-generation-web-ui" title="Daniel Pocock: Trialing the xTuple/PostBooks next generation web UI">
            <h2>Daniel Pocock: Trialing the xTuple/PostBooks next generation web UI</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 04, 2014</span>.
            
        </p>

        <div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even"><p>For some time I've been using <a href="http://danielpocock.com/postbooks-packages-available">PostBooks</a> to keep track of finances.  The traditional PostBooks system has a powerful <a href="http://en.wikipedia.org/wiki/Qt_%28software%29">Qt</a> GUI.</p>
<p>The xTuple team have been hard at work creating a shiny new web-based user interface.</p>
<p>The traditional UI has no dedicated server - all users communicate directly with the <a href="http://www.postgresql.org/">PostgreSQL database</a> where stored procedures and triggers ensure the correct logic is applied.</p>
<p><img src="http://danielpocock.com/sites/danielpocock.com/files/postbooks-scaled.png" /></p>
<p>The new model provides an xTuple application server that can handle requests from web users and potentially other third-party apps too.</p>
<h3>Who is it for?</h3>
<p>Some people may feel that the web UI is intended to appeal to mobile users.  While it is useful for mobile and tablet devices, this is not strictly the aim, <a href="http://www.xtuple.org/blog/jrogelstad/xtuple-web-client-its-not-just-mobile-devices">John has discussed this in a blog</a>.</p>
<p>One benefit of the web UI is that accountants and book-keepers do not need to have a copy of every exact PostBooks version that every client is using.  Given that many people only need their accountant to look at their books for just a few hours at the end of each year, the ease of access with a web UI will make a big difference.</p>
<h3>Trying it out quickly</h3>
<p>The xTuple Git repository <a href="https://github.com/xtuple/xtuple/blob/master/scripts/install_xtuple.sh">provides a script to install the whole server quickly</a>.  Initially it just supported a single Ubuntu release, I just contributed some tweaks to generalize it for Debian wheezy and potentially other releases.  It doesn't appear too difficult to generalize it further for Fedora or RHEL users.</p>
<p>To get going, I recommend trying it in a fresh virtual machine, either in a server environment or desktop VirtualBox solution.  The installation script will install various packages on the machine and mess about with the PostgreSQL setup so you will not want to run the automated setup script on any machine where you have existing databases.</p>
<p>Once the virtual machine is setup, make sure <em>sudo</em> is installed and configured:</p>
<p><code><br />
# apt-get install sudo<br />
# visudo<br /></code></p>
<p>and then run the install as your normal user:</p>
<p><code><br />
git clone --recursive git://github.com/xtuple/xtuple.git<br />
cd xtuple<br />
git remote add XTUPLE git://github.com/xtuple/xtuple.git<br />
git fetch XTUPLE<br />
git checkout `git describe --abbrev=0`<br />
chmod a+x scripts/install_xtuple.sh<br />
scripts/install_xtuple.sh<br /></code></p>
<p>If all goes well, 5-10 minutes later it is ready to run:</p>
<p><code><br />
cd node-datasource<br />
node main.js<br /></code></p>
<p>The port numbers will appear on the screen and you can connect with a web browser.</p>
<h3>Trying it out</h3>
<p>Despite my comments above to the effect that this is not primarily aimed at mobile, the first and second device I tested with were both mobile devices, Samsung Galaxy S3 and a Samsung Galaxy Note 3.  I feel the Note is far better for this type of application, primarily due to screen size and the fact that most of the forms in the application have fields that launch popup menus.  It appears to work in both Chrome and Firefox on these devices.</p>
<p>One handy feature is that the mobile device can dial numbers directly from the CRM address book, this is facilitated with the <a href="http://danielpocock.com/click-to-dial-for-mobile-users-of-your-web-sites">tel URI</a>.</p>
<p>My impression is that this is still a product that is in the final stages of development, although some people will be able to use it almost immediately.  One significant thing to note is that the database schema is very stable due to the long history of the traditional xTuple/PostBooks products.</p>
</div></div></div>
    </div>
    <div class="feedEntry">

        <a href="http://citusdata.com/blog/78-postgresql-columnar-store-benchmarks-on-ssds" title="Hadi Moshayedi: PostgreSQL columnar store benchmarks on SSDs">
            <h2>Hadi Moshayedi: PostgreSQL columnar store benchmarks on SSDs</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 04, 2014</span>.
            
        </p>

        <p>
In April we released cstore_fdw, the first columnar store foreign data wrapper for PostgreSQL. Our <a href="http://citusdata.com/blog/76-postgresql-columnar-store-for-analytics">initial blog post</a> received lots of interest in cstore_fdw but also lots questions. In this blog post we're going to attempt to answer some of the more common performance related questions:
</p>

<ul>
    <li>How much disk space does compression save me?</li>
    <li>How much faster are queries using cstore_fdw?</li>
    <li>How much do cstore_fdw skiplist indexes improve performance?</li>
</ul>
<p>
Before we start we will briefly touch on the test environment. All of the tests below were done using data and some representative queries generated by the <a href="http://www.tpc.org/tpch/">industry standard TPC-H benchmark</a>. We ran our tests on EC2 hosts using 10 GB of generated TPC-H data loaded into CitusDB version 3.0 or PostgreSQL version 9.3.4. All database files were stored on SSD drives. CitusDB and PostgreSQL were configured identically with the only variations from the stock configuration being setting shared_buffers to 2 GB and work_mem to 512 MB.
</p>

<h3>How much disk space does compression save me?</h3>

<p>
cstore_fdw allows users to enable compression on their tables to reduce the size of the data at rest on disk and to reduce the amount of I/O required to read the table. We won't dive into the details of the compression technique used in this post but a short description is that each column of data is broken up into blocks of ten thousand values which are compressed using PostgreSQL's built-in LZ compression. Compression has four primary benefits:
</p>

<ul>
    <li>You spend less money on expensive SSDs to store your data.</li>
    <li>You read less from your disks freeing up limited disk I/O for other tasks.</li>
    <li>Smaller files are more likely to fit into buffer cache which significantly improves query times.</li>
    <li>Disk access is often a performance bottleneck and by reading less from them you improve query times. </li>
</ul>
<br />
<p>
In order to test how well our compression technique works we took 10 GB of raw data generated by TPC-H and loaded that into a standard PostgreSQL database and into a PostgreSQL database that uses compressed cstore_fdw tables. We then compared the sizes of the tables in each of the databases. The table below summarizes the compression ratios we saw for each table:
</p>
<table><thead>
<tr>
<th>Table Type</th>
<th>Lineitem (9.1 GB)</th>
<th>Orders (2.1 GB)</th>
<th>Part (0.3 GB)</th>
<th>Partsupp (1.4 GB)</th>
<th>Customer (0.3 GB)</th>
</tr>
</thead><tbody>
<tr>
<td>Regular</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>cstore</td>
<td>0.98</td>
<td>0.87</td>
<td>0.96</td>
<td>0.82</td>
<td>0.93</td>
</tr>
<tr>
<td>cstore (compressed)</td>
<td>0.26</td>
<td>0.27</td>
<td>0.22</td>
<td>0.26</td>
<td>0.40</td>
</tr>
</tbody></table>
<figure class="chart" style="margin-bottom: 1cm; margin-top: 1cm;" title="cstore_fdw Compression Rates">
  <div title="Table Size (normalized to 1.0)"></div>
  <figcaption>Generated TPC-H data with scale factor 10</figcaption>
</figure>

<p>
Depending on the TPC-H table compression ratios varied between 2.5x and 16x. This type of variability is expected depending on the size of the table and the type of the data contained in it. One other interesting note is that our 13.1 GB PostgreSQL database compressed down only 3.5 GB when using our compressed storage format.
</p>

<p>
Using compression with cstore_fdw we would expect users to see compression ratios of 3-4x for their tables.
</p>

<h3>How much faster is cstore_fdw relative to PostgreSQL?</h3>

<p>
cstore_fdw improves query times by reducing the amount of I/O in two ways:
</p>

<ul>
    <li>Reading data only for columns needed to answer a query as opposed to reading all columns in each row.</li>
    <li>Compressing data on disk.</li>
</ul>

<p>
We wanted to evaluate how much of an improvement in query times users of cstore_fdw were likely to see in the real world. To do this we used the data and queries from the TPC-H benchmark as these are designed to represent real-world use cases. Our test consisted of loading 10 GB of generated TPC-H data into both a standard PostgreSQL database and a PostgreSQL database that used cstore_fdw storage. In order to test performance we ran a selection of the TPC-H queries against each of the databases and recorded the query times:
</p>

<table><thead>
<tr>
<th>Table Type</th>
<th>3</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>10</th>
<th>12</th>
<th>14</th>
<th>19</th>
</tr>
</thead><tbody>
<tr>
<td>PostgreSQL</td>
<td>36.78</td>
<td>34.37</td>
<td>25.92</td>
<td>36.17</td>
<td>37.42</td>
<td>36.55</td>
<td>33.86</td>
<td>31.91</td>
<td>28.60</td>
</tr>
<tr>
<td>cstore</td>
<td>26.60</td>
<td>28.34</td>
<td>15.75</td>
<td>29.03</td>
<td>32.15</td>
<td>27.88</td>
<td>26.28</td>
<td>15.50</td>
<td>33.00</td>
</tr>
<tr>
<td>cstore (compressed)</td>
<td>25.84</td>
<td>27.68</td>
<td>14.28</td>
<td>28.28</td>
<td>31.26</td>
<td>25.74</td>
<td>24.70</td>
<td>14.88</td>
<td>25.54</td>
</tr>
</tbody></table>
<figure class="chart" style="margin-bottom: 1cm; margin-top: 1cm;" title="TPC-H Query Times">
  <div title="Query Time (seconds)"></div>
  <figcaption>Run on PostgreSQL 9.3.4</figcaption>
</figure>

<p>
As shown above the improvement from cstore_fdw varies depending on the query. In the best case query runtimes dropped to half their previous levels, in the worst case the improvement is only 11% (with compression enabled). 
</p>

<p>
Explaining this variation requires a bit of an explanation of how databases work. The total query time can be thought of as the sum of two types of work: time spent doing I/O and time spent doing computation. Queries that use complex filters, aggregations, or other functions spend increased amounts of time doing computation. For these types of queries it can be difficult to significantly affect overall query time by reducing I/O as the majority of the query time is spent doing non-I/O related work. However, for many common queries there is relatively little computation and reducing I/O does have a significant impact on overall query times.
</p>

<p>
Across the set of TPC-H queries we see that when using cstore_fdw with compression every query executes faster and users can expect their analytic queries run 20-30% faster. We'll show how to turn this 20-30% improvement into a 60-70% one later in our investigation on skiplist indexes.
</p>

<h3>How much faster is cstore_fdw relative to CitusDB?</h3>

<p>
CitusDB enables its users to create a horizontally scalable PostgreSQL database that can leverage the PostgreSQL extension ecosystem. To determine the benefits of cstore_fdw for CitusDB users we set up an experiment identical to the one we did for PostgreSQL except this time we compared two CitusDB clusters; one using PostgreSQL's native storage format for the TPC-H data and one using cstore_fdw to store the same TPC-H data.
</p>

<table><thead>
<tr>
<th>Table Type</th>
<th>3</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>10</th>
<th>12</th>
<th>14</th>
<th>19</th>
</tr>
</thead><tbody>
<tr>
<td>CitusDB</td>
<td>35.87272</td>
<td>35.617836</td>
<td>25.683736</td>
<td>35.915007</td>
<td>39.537044</td>
<td>40.360714</td>
<td>31.864123</td>
<td>29.510186</td>
<td>29.593161</td>
</tr>
<tr>
<td>CitusDB + cstore</td>
<td>13.664943</td>
<td>13.34371</td>
<td>6.176959</td>
<td>14.522777</td>
<td>18.638935</td>
<td>17.483885</td>
<td>9.619221</td>
<td>9.401148</td>
<td>17.571072</td>
</tr>
<tr>
<td>CitusDB + cstore (compressed)</td>
<td>12.476285</td>
<td>12.060501</td>
<td>5.600474</td>
<td>12.884196</td>
<td>17.033522</td>
<td>16.009109</td>
<td>7.595766</td>
<td>8.272748</td>
<td>12.080371</td>
</tr>
</tbody></table>
<figure class="chart" style="margin-bottom: 1cm; margin-top: 1cm;" title="TPC-H Query Times">
  <div title="Query Time (seconds)"></div>
  <figcaption>Run on CitusDB 3.0</figcaption>
</figure>

 
<p>
The results are similar to those when using cstore_fdw on PostgreSQL, however there is a significant difference in the amount of improvement seen. When using PostgreSQL we saw an average improvement to query times of 28% whereas when using CitusDB we see an average improvement to query times of 66%. This occurs because CitusDB enables you simultaneously use all cores on a machine to answer a query. In doing so CitusDB introduces more demand and hence more contention for limited available disk I/O. As a result the reduction in I/O times that cstore_fdw brings contributes more significantly to improving query times. Users running cstore_fdw on PostgreSQL in an environment with multiple clients querying in parallel should expect similar results.
</p>

<p>
cstore_fdw users with CitusDB can expect to see their performance improve by 60-70% for their analytic query workloads.
</p>

<h3>How much do cstore_fdw skiplist indexes improve performance?</h3>

<p>
Indexing is a common technique for improving query performance in databases and PostgreSQL has a number of indexing techniques available for its users. However, when using a foreign data wrapper, like cstore_fdw, users can no longer leverage PostgreSQL's built-in indexing techniques. In order to give users of cstore_fdw the benefits of indexes we adapted the indexing technology from <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html">ORC</a>. The ORC indexing system works by breaking each column's data into blocks and storing the min and max values of each block. In order to determine how much of an improvement this indexing technique offers we used the TPC-H data loaded onto a couple PostgreSQL databases using cstore_fdw. The most effective use of ORC's indexing system requires that the column data be sorted. Into one of these databases we loaded sorted data and into the other unsorted data was loaded. As previously we then ran the TPC-H queries against each of these databases. It's worth noting that all data was sorted by its natural ordering, meaning for example that orders were sorted by order date.
</p>

<p>
One thing to note about these results is that the tests were done without compression enabled. Users should expect to see additional improvements from enabling compression as described in previous sections.
</p>

<table><thead>
<tr>
<th>Table Type</th>
<th>3</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>10</th>
<th>12</th>
<th>14</th>
<th>19</th>
</tr>
</thead><tbody>
<tr>
<td>cstore</td>
<td>34.961275</td>
<td>36.959757</td>
<td>15.956601</td>
<td>37.265958</td>
<td>40.437036</td>
<td>33.260259</td>
<td>29.418966</td>
<td>17.442527</td>
<td>36.130426</td>
</tr>
<tr>
<td>cstore (sorted)</td>
<td>26.47118</td>
<td>34.780422</td>
<td>4.620994</td>
<td>26.371364</td>
<td>38.380606</td>
<td>21.975958</td>
<td>13.398782</td>
<td>3.646738</td>
<td>37.277029</td>
</tr>
</tbody></table>
<figure class="chart" style="margin-bottom: 1cm; margin-top: 1cm;" title="TPC-H Query Times">
  <div title="Query Time (seconds)"></div>
  <figcaption>Run on PostgreSQL 9.3.4</figcaption>
</figure>


<p>
When using cstore_fdw with sorted input users should expect a 30-35% improvement to query times compared to using unsorted input and a 60-70% improvement compared to PostgresSQL.
</p>

<h3>Conclusions</h3>

<p>
After benchmarking cstore_fdw in a number of analytic query scenarios we learned:
</p>
<ul>
    <li>cstore_fdw users can expect to see their databases shrink by 3-4x</li>
    <li>cstore_fdw users with PostgreSQL can expect to see their query times improve by 20-30%</li>
    <li>cstore_fdw users with CitusDB can expect to see their query times improve by 60-70%</li>
    <li>cstore_fdw users can use indexing for a further ~30% improvement to query times</li>
</ul>
<p>
We hope this post left you interested to see what cstore_fdw and CitusDB can do for your data. Feel free to contact us at engage @ citusdata.com if you have any questions.
</p>

<p>
To download cstore_fdw and Citus DB see these links: <a href="https://github.com/citusdata/cstore_fdw">cstore_fdw</a> and <a href="http://citusdata.com/downloads">CitusDB</a>.
</p>
    </div>
    <div class="feedEntry">

        <a href="http://www.databasesoup.com/2014/06/upcoming-sfpug-live-streams-crontabber.html" title="Josh Berkus: Upcoming SFPUG Live Streams: Crontabber and JSONB">
            <h2>Josh Berkus: Upcoming SFPUG Live Streams: Crontabber and JSONB</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 03, 2014</span>.
            
        </p>

        Apparently June is the month for hackers named Peter with difficult-to-spell last names.&nbsp; Join us for two live streams of two different Peters giving two different PostgreSQL presentations.<br /><br />First, on June 10th, we'll have <a href="http://www.meetup.com/postgresql-1/events/182721612/" target="_blank">Peter Bengtsson of Mozilla presenting Crontabber</a>, a network-wide scheduled job manager written in Python and PostgreSQL.&nbsp; <a href="https://air.mozilla.org/crontabber/" target="_blank">Tune in on Air Mozilla</a> at around 7:15pm PDT to watch this.<br /><br />Second, on June 23rd, Peter Geoghegan of Heroku will do a "<a href="http://www.meetup.com/postgresql-1/events/183838102/" target="_blank">JSONB Deep Dive</a>" and explain the features and internals of the new JSONB type.&nbsp;&nbsp; <a href="https://plus.google.com/events/cj6s3nus1ul24rtenhfcds8oksg" target="_blank">Subscribe to the Google Event</a> to be linked into the Google Hangout when this goes live around 7:15PM PDT.<br /><br />See you there!<br /><br /><br />
    </div>
    <div class="feedEntry">

        <a href="http://feedproxy.google.com/~r/blogspot/rFRqt/~3/8KCE4eucpUE/32-bit-postgresql-compilation-on-64-bit.html" title="Dinesh Kumar: 32-bit PostgreSQL Compilation On 64-bit CentOS 6.x">
            <h2>Dinesh Kumar: 32-bit PostgreSQL Compilation On 64-bit CentOS 6.x</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">Jun 03, 2014</span>.
            
        </p>

        I am sure that, most of you aware of this. But, for me it's the first time, i accomplished it.<br /><br />As one of my assigned tasks to build a 32-bit instance of postgresql on 64-bit machine, i have followed the below approach. I hope, it will be helpful to others as well, if you got any problems.<br /><br />As a initial step on this task, i have tried to build a sample "c" program using "gcc -m32". Once, i resolved this, i moved to compile the PostgreSQL 9.0.<br /> <pre class="prettyprint"><br />[root@localhost Desktop]# gcc -m32 -o test test.c<br />In file included from /usr/include/features.h:385,<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;from /usr/include/stdio.h:28,<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;from test.c:1:<br />/usr/include/gnu/stubs.h:7:27: error: gnu/stubs-32.h: No such file or directory<br /></pre>To resolve the above issue, i have installed the 32-bit glibc-devel package through yum. <pre class="prettyprint"><br />yum -y install glibc-devel.i686 glibc-devel<br /></pre> Again, i have tried to run the same command. <pre class="prettyprint"><br />[root@localhost Desktop]# gcc -m32 -o test test.c<br />/usr/bin/ld: skipping incompatible /usr/lib/gcc/x86_64-redhat-linux/4.4.6/libgcc_s.so when searching for -lgcc_s<br />/usr/bin/ld: skipping incompatible /usr/lib/gcc/x86_64-redhat-linux/4.4.6/libgcc_s.so when searching for -lgcc_s<br />/usr/bin/ld: cannot find -lgcc_s<br />collect2: ld returned 1 exit status<br /></pre> Now, i got a different error message, and tried to install the 32-bit libgcc.  <pre class="prettyprint"><br />[root@localhost Desktop]# yum install libgcc-*.i686<br />--&gt; Running transaction check<br />---&gt; Package libgcc.i686 0:4.4.7-4.el6 will be installed<br />--&gt; Finished Dependency Resolution<br />Error: Protected multilib versions: libgcc-4.4.7-4.el6.i686 != libgcc-4.4.6-3.el6.x86_64<br /></pre> As it's complaining, we have the old version of x86_64, when compared the new one. Hence, i have tried to update the existing x86_64.  <pre class="prettyprint"><br />[root@localhost Desktop]# yum update libgcc-4.4.6-3.el6.x86_64<br />---&gt; Package libgcc.x86_64 0:4.4.6-3.el6 will be updated<br />---&gt; Package libgcc.x86_64 0:4.4.7-4.el6 will be an update<br />--&gt; Finished Dependency Resolution<br /></pre> Once, it's updated the given library, i have again tried to install the libgcc-*.i686.  <pre class="prettyprint"><br />[root@localhost Desktop]# yum install libgcc-*.i686<br />Resolving Dependencies<br />--&gt; Running transaction check<br />---&gt; Package libgcc.i686 0:4.4.7-4.el6 will be installed<br />--&gt; Finished Dependency Resolution<br /></pre> Now, i am trying to run the same "gcc -m32" command to check for any further issues.  <pre class="prettyprint"><br />[root@localhost Desktop]# gcc -m32 -o test test.c<br />[root@localhost Desktop]# ./test<br />Hello World<br /></pre> It looks, the sample "c program" is working fine, as a 32-bit application.  Now, i am moving to PostgreSQL 9.0. As per my observation i have updated, installed the below components for the PostgreSQL.  <pre class="prettyprint"><br />yum update readline-6.0-3.el6.x86_64<br />yum install *readline*i686<br />yum update zlib-1.2.3-27.el6.x86_64<br />yum install *zlib*i686<br /></pre> Once, i got the all required 32-bit libraries, i have tried to compile the postgresql as below.  <pre class="prettyprint"><br />[root@localhost build]# CFLAGS=-m32 LDFLAGS=-m32 CXXFLAGS=-m32 ./configure --prefix=/opt/PostgreSQL/build<br />[root@localhost build]# make -j 4; make install;<br />....<br />make[1]: Leaving directory `/root/Downloads/postgresql-9.0.17/config'<br />PostgreSQL installation complete.<br /></pre> It seems, postgresql has built successfully on 64-bit machine as a 32-bit application.  Check for the confirmation from OS, PG.  <pre class="prettyprint"><br />postgres=# SELECT version();<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; version &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br />-------------------------------------------------------------------------------------------------------------------<br />&nbsp;PostgreSQL 9.0.17 on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit<br />(1 row)<br /><br />[root@localhost build]# file /sbin/init<br />/sbin/init: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped<br /></pre> And, at the end, i have the postgresql as 32-bit application in 64-bit machine. <br /><br />Thank you for reading, and please comment on this, if you have any questions. <br /><br /><br />--Dinesh Kumar<img height="1" src="http://feeds.feedburner.com/~r/blogspot/rFRqt/~4/8KCE4eucpUE" width="1" />
    </div>
    <div class="feedEntry">

        <a href="http://www.postgresonline.com/journal/archives/332-psql-watch-for-batch-processing.html" title="Leo Hsu and Regina Obe: psql watch for batch processing">
            <h2>Leo Hsu and Regina Obe: psql watch for batch processing</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">May 30, 2014</span>.
            
        </p>

        <p>A while back, we discussed using <a href="http://www.postgresonline.com/journal/archives/181-pgAdmin-pgScript.html" target="_blank">pgAdmin pgScript</a> as a quicky way for
running a repetitive update script where you want each loop to commit right away. Since stored functions have to commit as a whole, you can't use stored functions alone for this kind of processing. </p>

<b>Question: Can you do similar easily with psql?</b>
<br />
<b>Answer:</b> yes with the <a href="http://michael.otacoo.com/postgresql-2/postgres-9-3-feature-highlight-watch-in-psql" target="_blank">\watch command described nicely by Michael Paquier</a> a while back.<br />
<p>If you are using the psql client packaged with PostgreSQL 9.3 or above,
then you can take advantage of the <code>\watch</code> command that was introduced in that version of psql.  We'll demonstrate that
by doing a batch geocoding exercise with PostGIS tiger geocoder and also revise our example from the prior article to use the more efficient and terser LATERAL  construct introduced in PostgreSQL 9.3.</p> <br /><a href="http://www.postgresonline.com/journal/archives/332-psql-watch-for-batch-processing.html#extended">Continue reading "psql watch for batch processing"</a>
    </div>
    <div class="feedEntry">

        <a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-logical-replication-receiver/" title="Michael Paquier: Postgres 9.4 feature highlight: Logical replication receiver">
            <h2>Michael Paquier: Postgres 9.4 feature highlight: Logical replication receiver</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">May 30, 2014</span>.
            
        </p>

        <p>These last couple of days I have been developing for studies a background
worker able to fetch changes from a logical decoder already developed
and presented on this blog called <a href="https://github.com/michaelpq/pg_plugins/tree/master/decoder_raw">decoder_raw</a> able
to generate raw queries based on the logical changes decoded on server
from its WAL. This receiver, called <a href="https://github.com/michaelpq/pg_plugins/tree/master/receiver_raw">receiver_raw</a>
runs as a background worker that connects to a node running decoder_raw
using a replication connection and then fetches decoded logical changes
from it, which are in this case ready-to-be-applied raw queries. Those
changes are then applied on the database one by one using the <a href="http://www.postgresql.org/docs/devel/static/spi.html">SPI</a> through  a loop
process that sleeps during a customizable amount of nap time. Note that
receiver_raw is not <em>that</em> performant as it only applies one single
query per process loop, and loop sleeps for a given amount of time which
is of course customizable with a GUC. More complicated processing like
applying batches of changes could be done as well but as a proof of concept
this plugin is enough as it is.</p>

<p>By the way, here are a couple of things to be aware of when developing
your own logical change receiver.</p>

<h4 id="toc_0">Connection string</h4>

<p>The <a href="http://www.postgresql.org/docs/devel/static/libpq-connect.html#LIBPQ-CONNSTRING">connection string</a>
that is being used to connect to the remote node should have the following
shape at minimum to set up a replication connection:</p>
<div class="highlight"><pre><code class="text language-text">replication=database dbname=postgres
</code></pre></div>
<p>Then it is only a matter of calling PQconnectdb or PQconnectdbParams to
connect to the database.</p>

<h4 id="toc_1">Initialization of logical replication</h4>

<p>There are a couple of things to remember here:</p>

<ul>
<li>It is possible to precise 0/0 as a start position for the logical
replication to let the server manage the start point.</li>
<li>Passing options is possible at this point to customize the data
received from the decoder or to put restrictions on it.</li>
<li>Initialization is done with the COPY protocol</li>
</ul>

<p>You would then finish with something like that:</p>
<div class="highlight"><pre><code class="text language-text">PQExpBuffer query;
PGconn *conn;
PGresult *res;

/* Query buffer for remote connection */
query = createPQExpBuffer();

/* Start logical replication at specified position */
appendPQExpBuffer(query, &quot;START_REPLICATION SLOT \&quot;%s\&quot; LOGICAL 0/0 &quot;
                         &quot;(\&quot;include_transaction\&quot; 'off')&quot;,
                  receiver_slot);
res = PQexec(conn, query-&gt;data);
if (PQresultStatus(res) != PGRES_COPY_BOTH)
{
    PQclear(res);
    ereport(LOG, (errmsg(&quot;Could not start logical replication&quot;)));
    proc_exit(1);
}
PQclear(res);
resetPQExpBuffer(query);
</code></pre></div>
<h4 id="toc_2">Fetching changes</h4>

<p>Now that the initialization has been done, you can enter in the main
processing loop and fetch the changes. This consists simply in calling
PQgetCopyData to fetch a single change and then to do some processing
depending on the quantity of data fetched:</p>

<ul>
<li>0, nothing has been fetched, so move on to the next loop</li>
<li>-1, the end of the COPY stream</li>
<li>-2, an error occurred</li>
<li>In all the other cases there is some data to treat</li>
</ul>

<p>When there is data the receiver can get two types of message:</p>

<ul>
<li>'k' for a keepalive message. Receiver should send feedback at this
point.</li>
<li>'w' for a stream message with some change data in it.</li>
</ul>

<p>Both message follow a particular format, and contain information about
the WAL position of remote server or the time when change has been sent
for example. Using Postgres core code, having a look at pg_recvlogical
is a good start. Note that receiver_raw does its job as well.</p>

<h4 id="toc_3">Strengthen inter-node communication with WAL position feedback</h4>

<p>It is really important to have the logical receiver tell back to the
remote server what is the LSN position (or WAL position if you want)
that it has already written and flushed. If the receiver does not do
that, remote server will retain WAL files that are perhaps not
necessary anymore, blowing the amount of space dedicated to WAL, something
particularly painful when WAL files are on a dedicated partition whose
size may be limited. Also when the receiver reinitializes a logical
replication protocol, past changes will be fetched again... The best
thing to have a receiver send feedback is to have a look at the function
called SendFeedback in src/bin/pg_basebackup/receivelog.c, and simply
copy/paste it to your code. You won't regret it.</p>

<h4 id="toc_4">An example</h4>

<p>Now is finally game time, with a simple example using two nodes. A first
node listening to port 5432 runs the logical decoder decoder_raw.
A logical slot has been created on it, under the database &quot;postgres&quot;.</p>
<div class="highlight"><pre><code class="text language-text">$ psql -p 5432 postgres \
   -c 'SELECT slotname FROM pg_create_logical_replication_slot('slot', 'decoder_raw');
 slotname 
----------
 slot
(1 row)
</code></pre></div>
<p>And of course the second node runs the logical receiver receiver_raw.</p>

<p>Both nodes use the same schema, a simple table with a primary key:</p>
<div class="highlight"><pre><code class="text language-text">$ psql -c '\d aa' postgres
      Table &quot;public.aa&quot;
 Column |  Type   | Modifiers 
--------+---------+-----------
 a      | integer | not null
Indexes:
    &quot;aa_pkey&quot; PRIMARY KEY, btree (a)
</code></pre></div>
<p>Using the whole set, do the changes get replicated? Obviously the
answer is yes or this post has no meaning, here are some tuples inserted
on node 1...</p>
<div class="highlight"><pre><code class="text language-text">$ psql -c 'INSERT INTO aa VALUES (generate_series(1,10))' postgres
INSERT 0 10
</code></pre></div>
<p>... Getting replicated on node 2...</p>
<div class="highlight"><pre><code class="text language-text">$ psql -p 5433 -c 'SELECT count(*) FROM aa' postgres
 count 
-------
    10
(1 row)
</code></pre></div>
<p>Feel free to test the two plugins used in this example for your own
needs. A direct application of those things would be to create a
set of nodes replicating changes in circle, the only challenge being
to be sure to track what is the node from which the change comes from
at then to stop applying the change once it comes back to its origin.
This would need some kind of node origin tracker. Constraint validation
is of course another story...</p>

<p>Note that this is the last post dedicated to the introduction of logical
replication for 9.4, feel free to refer to the previous entries of the
series as well:</p>

<ul>
<li><a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-replication-slots/">More about replication slots</a></li>
<li><a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-basics-logical-decoding/">Basics about logical replication</a></li>
<li><a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-replica-identity-logical-replication/">REPLICA IDENTITY</a></li>
<li><a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-output-plugin-logical-replication/">About logical decoding plugins</a></li>
<li><a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-logical-replication-protocol/">Logical replication protocol</a></li>
</ul>

<p>The documentation of Postgres itself about <a href="http://www.postgresql.org/docs/devel/static/logicaldecoding.html">logical decoding</a> is of
course highly recommended.</p>
    </div>
    <div class="feedEntry">

        <a href="http://www.keithf4.com/table-partitioning-and-foreign-keys/" title="Keith Fiske: Table Partitioning and Foreign Keys">
            <h2>Keith Fiske: Table Partitioning and Foreign Keys</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">May 28, 2014</span>.
            
        </p>

        <p>Table partitioning &amp; foreign keys don&#8217;t get along very well in databases and PostgreSQL&#8217;s lack of having it built in shows it very clearly with the workarounds that are necessary to avoid the issues. The latest release of <a href="https://github.com/keithf4/pg_partman" target="_blank">pg_partman</a> deals with the lesser of two shortcomings that must be dealt with, that being where child tables in a partition set do not automatically inherit foreign keys created on the parent table. I&#8217;ll be using my other extension <a href="https://github.com/omniti-labs/pg_jobmon" target="_blank">pg_jobmon</a> as a reference for example here since it works well to illustrate both the issues and possible solutions. You can see here the job_detail table, which contains the individual steps of a logged job, references the the job_log table for the main job_id values.</p><pre class="crayon-plain-tag">keith=# \d jobmon.job_detail
                                           Table "jobmon.job_detail"
    Column    |           Type           |                              Modifiers                              
--------------+--------------------------+---------------------------------------------------------------------
 job_id       | bigint                   | not null
 step_id      | bigint                   | not null default nextval('jobmon.job_detail_step_id_seq'::regclass)
 action       | text                     | not null
 start_time   | timestamp with time zone | not null
 end_time     | timestamp with time zone | 
 elapsed_time | real                     | 
 status       | text                     | 
 message      | text                     | 
Indexes:
    "job_detail_step_id_pkey" PRIMARY KEY, btree (step_id)
    "job_detail_job_id_idx" btree (job_id)
Foreign-key constraints:
    "job_detail_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id) ON DELETE CASCADE</pre><p>With version &lt;= 1.7.0 of pg_partman, turning this table into a partition set illustrates the issue.</p><pre class="crayon-plain-tag">keith=# select partman.create_parent('jobmon.job_detail', 'job_id', 'id-static', '10000', p_jobmon := false);
 create_parent 
---------------
 
(1 row)

keith=# \d+ jobmon.job_detail
                                                               Table "jobmon.job_detail"
    Column    |           Type           |                              Modifiers                              | Storage  | Stats target | Description 
--------------+--------------------------+---------------------------------------------------------------------+----------+--------------+-------------
 job_id       | bigint                   | not null                                                            | plain    |              | 
 step_id      | bigint                   | not null default nextval('jobmon.job_detail_step_id_seq'::regclass) | plain    |              | 
 action       | text                     | not null                                                            | extended |              | 
 start_time   | timestamp with time zone | not null                                                            | plain    |              | 
 end_time     | timestamp with time zone |                                                                     | plain    |              | 
 elapsed_time | real                     |                                                                     | plain    |              | 
 status       | text                     |                                                                     | extended |              | 
 message      | text                     |                                                                     | extended |              | 
Indexes:
    "job_detail_step_id_pkey" PRIMARY KEY, btree (step_id)
    "job_detail_job_id_idx" btree (job_id)
Foreign-key constraints:
    "job_detail_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id) ON DELETE CASCADE
Triggers:
    job_detail_part_trig BEFORE INSERT ON jobmon.job_detail FOR EACH ROW EXECUTE PROCEDURE jobmon.job_detail_part_trig_func()
Child tables: jobmon.job_detail_p0,
              jobmon.job_detail_p10000,
              jobmon.job_detail_p20000,
              jobmon.job_detail_p30000,
              jobmon.job_detail_p40000
Has OIDs: no

keith=# \d+ jobmon.job_detail_p0
                                                             Table "jobmon.job_detail_p0"
    Column    |           Type           |                              Modifiers                              | Storage  | Stats target | Description 
--------------+--------------------------+---------------------------------------------------------------------+----------+--------------+-------------
 job_id       | bigint                   | not null                                                            | plain    |              | 
 step_id      | bigint                   | not null default nextval('jobmon.job_detail_step_id_seq'::regclass) | plain    |              | 
 action       | text                     | not null                                                            | extended |              | 
 start_time   | timestamp with time zone | not null                                                            | plain    |              | 
 end_time     | timestamp with time zone |                                                                     | plain    |              | 
 elapsed_time | real                     |                                                                     | plain    |              | 
 status       | text                     |                                                                     | extended |              | 
 message      | text                     |                                                                     | extended |              | 
Indexes:
    "job_detail_p0_pkey" PRIMARY KEY, btree (step_id)
    "job_detail_p0_job_id_idx" btree (job_id)
Check constraints:
    "job_detail_p0_partition_check" CHECK (job_id &gt;= 0::bigint AND job_id &lt; 10000::bigint)
Inherits: jobmon.job_detail
Has OIDs: no</pre><p>You can see it is now a partitioned table, but if you look at any of the children, none of them have the FK back to the main job_log table.</p>
<p>As a side note, notice I set the p_jobmon parameter to false in create_parent(). By default pg_partman uses pg_jobmon when it is installed to log everything it does and provide monitoring that your partitioning is working. Since this would mean pg_jobmon is trying to log the partitioning steps of its own table, it puts it into a permanent lockwait state since it&#8217;s trying to write to the table it is partitioning. Turning pg_jobmon off for the initial creation avoids this compatibility issue between these two extensions. It can be turned back on for monitoring of future child table creation by modifying the <em>jobmon </em>column in pg_partman&#8217;s part_config table. Creation of partitions ahead of the current one does not interfere since a lock on the parent table is no longer required.</p>
<p>Back to the foreign key issue&#8230; Lets undo the partitioning here, upgrade pg_partman, and try again</p><pre class="crayon-plain-tag">keith=# select partman.undo_partition_id('jobmon.job_detail', 20, p_keep_table := false);
NOTICE:  Copied 0 row(s) to the parent. Removed 5 partitions.
 undo_partition_id 
-------------------
                 0
(1 row)

keith=# alter extension pg_partman update to '1.7.1';
ALTER EXTENSION

keith=# select partman.create_parent('jobmon.job_detail', 'job_id', 'id-static', '10000', p_jobmon := false);
 create_parent 
---------------
 
(1 row)

keith=# \d jobmon.job_detail_p0
                                         Table "jobmon.job_detail_p0"
    Column    |           Type           |                              Modifiers                              
--------------+--------------------------+---------------------------------------------------------------------
 job_id       | bigint                   | not null
 step_id      | bigint                   | not null default nextval('jobmon.job_detail_step_id_seq'::regclass)
 action       | text                     | not null
 start_time   | timestamp with time zone | not null
 end_time     | timestamp with time zone | 
 elapsed_time | real                     | 
 status       | text                     | 
 message      | text                     | 
Indexes:
    "job_detail_p0_pkey" PRIMARY KEY, btree (step_id)
    "job_detail_p0_job_id_idx" btree (job_id)
Check constraints:
    "job_detail_p0_partition_check" CHECK (job_id &gt;= 0::bigint AND job_id &lt; 10000::bigint)
Foreign-key constraints:
    "job_detail_p0_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id)
Inherits: jobmon.job_detail</pre><p>Now our child table has the parent foreign key! The apply_foreign_keys() plpgsql function and the reapply_foreign_keys.py script that are part of the version 1.7.1 can actually be used on any table inheritance set, not just the ones managed by pg_partman. So some may find it useful elsewhere as well. So, what happens if we now partition the reference table, job_log, as well?</p><pre class="crayon-plain-tag">keith=# select partman.create_parent('jobmon.job_log', 'job_id', 'id-static', '10000', p_jobmon := false);
 create_parent 
---------------
 
(1 row)

keith=# \d+ jobmon.job_log
                                                             Table "jobmon.job_log"
   Column   |           Type           |                            Modifiers                            | Storage  | Stats target | Description 
------------+--------------------------+-----------------------------------------------------------------+----------+--------------+-------------
 job_id     | bigint                   | not null default nextval('jobmon.job_log_job_id_seq'::regclass) | plain    |              | 
 owner      | text                     | not null                                                        | extended |              | 
 job_name   | text                     | not null                                                        | extended |              | 
 start_time | timestamp with time zone | not null                                                        | plain    |              | 
 end_time   | timestamp with time zone |                                                                 | plain    |              | 
 status     | text                     |                                                                 | extended |              | 
 pid        | integer                  | not null                                                        | plain    |              | 
Indexes:
    "job_log_job_id_pkey" PRIMARY KEY, btree (job_id)
    "job_log_job_name_idx" btree (job_name)
    "job_log_pid_idx" btree (pid)
    "job_log_start_time_idx" btree (start_time)
    "job_log_status_idx" btree (status)
Referenced by:
    TABLE "jobmon.job_detail" CONSTRAINT "job_detail_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id) ON DELETE CASCADE
    TABLE "jobmon.job_detail_p0" CONSTRAINT "job_detail_p0_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id)
    TABLE "jobmon.job_detail_p10000" CONSTRAINT "job_detail_p10000_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id)
    TABLE "jobmon.job_detail_p20000" CONSTRAINT "job_detail_p20000_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id)
    TABLE "jobmon.job_detail_p30000" CONSTRAINT "job_detail_p30000_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id)
    TABLE "jobmon.job_detail_p40000" CONSTRAINT "job_detail_p40000_job_id_fkey" FOREIGN KEY (job_id) REFERENCES jobmon.job_log(job_id)
Triggers:
    job_log_part_trig BEFORE INSERT ON jobmon.job_log FOR EACH ROW EXECUTE PROCEDURE jobmon.job_log_part_trig_func()
    trg_job_monitor AFTER UPDATE ON jobmon.job_log FOR EACH ROW EXECUTE PROCEDURE jobmon.job_monitor()
Child tables: jobmon.job_log_p0,
              jobmon.job_log_p10000,
              jobmon.job_log_p20000,
              jobmon.job_log_p30000,
              jobmon.job_log_p40000
Has OIDs: no


keith=# \d jobmon.job_log_p0
                                        Table "jobmon.job_log_p0"
   Column   |           Type           |                            Modifiers                            
------------+--------------------------+-----------------------------------------------------------------
 job_id     | bigint                   | not null default nextval('jobmon.job_log_job_id_seq'::regclass)
 owner      | text                     | not null
 job_name   | text                     | not null
 start_time | timestamp with time zone | not null
 end_time   | timestamp with time zone | 
 status     | text                     | 
 pid        | integer                  | not null
Indexes:
    "job_log_p0_pkey" PRIMARY KEY, btree (job_id)
    "job_log_p0_job_name_idx" btree (job_name)
    "job_log_p0_pid_idx" btree (pid)
    "job_log_p0_start_time_idx" btree (start_time)
    "job_log_p0_status_idx" btree (status)
Check constraints:
    "job_log_p0_partition_check" CHECK (job_id &gt;= 0::bigint AND job_id &lt; 10000::bigint)
Inherits: jobmon.job_log</pre><p>It partitions the table without any errors and you can see all the child table foreign keys referencing the parent. But notice the job_log_p0 child table? It has no references from any of the children. And this is the bigger issue that pg_partman does not solve, and most likely never will&#8230;</p>
<p><strong>Foreign key reference <em>checks</em> to the parent table in an inheritance set do not propagate to the children</strong></p>
<p>Since the parent table in an inheritance set is typically either empty, or only contains a fraction of the total data, the table referencing the partition set will either fail on every insert or when it hits a value that is only in a child table. The below SQL statements illustrate the issue</p><pre class="crayon-plain-tag">keith=# INSERT INTO jobmon.job_log (owner, job_name, start_time, pid) values ('keith', 'FK FAILURE TEST', now(), pg_backend_pid());
INSERT 0 0

keith=# select * from jobmon.job_log;
 job_id | owner |                     job_name                     |          start_time           | end_time | status |  pid  
--------+-------+--------------------------------------------------+-------------------------------+----------+--------+-------
      2 | keith | FK FAILURE TEST                                  | 2014-05-26 23:14:35.830266-04 | Â«NULLÂ»   | Â«NULLÂ» | 25286

keith=# insert into jobmon.job_detail (job_id, action, start_time) values (2, 'FK FAILURE TEST STEP 1', now());
ERROR:  insert or update on table "job_detail_p0" violates foreign key constraint "job_detail_p0_job_id_fkey"
DETAIL:  Key (job_id)=(2) is not present in table "job_log".
CONTEXT:  SQL statement "INSERT INTO jobmon.job_detail_p0 VALUES (NEW.*)"
PL/pgSQL function jobmon.job_detail_part_trig_func() line 11 at SQL statement</pre><p>You can clearly see the job_log table has the job_id value &#8220;2&#8243;, but trying to insert that value into the table that uses it as a reference fails. This is because that value lives in job_log_p0, not job_log and the FK reference check does not propagate to the child tables.</p><pre class="crayon-plain-tag">keith=# select * from only jobmon.job_log;
 job_id | owner | job_name | start_time | end_time | status | pid 
--------+-------+----------+------------+----------+--------+-----
(0 rows)

keith=# select * from only jobmon.job_log_p0;
 job_id | owner |    job_name     |          start_time           | end_time | status |  pid  
--------+-------+-----------------+-------------------------------+----------+--------+-------
      2 | keith | FK FAILURE TEST | 2014-05-26 23:14:35.830266-04 | Â«NULLÂ»   | Â«NULLÂ» | 25286
(1 row)</pre><p>I&#8217;m not sure of all of the reasons why PostgreSQL doesn&#8217;t allow FK checks to propagate down inheritance trees, but I do knowÂ one of the consequences of doing so could be some heavy performance hits for the source table if the inheritance set is very large. Every insert would have to scan down all tables in the inheritance tree. Even with indexes, this could be a very expensive.</p>
<p>There is a way to write a trigger and &#8220;fake&#8221; the foreign key if this is needed. I looked into this because I do want to be able to partition the pg_jobmon tables and keep referential integrity. To see how this works, I&#8217;m starting with a clean installation of pg_jobmon (no partitions). First the original foreign key on job_detail has to be removed, then a trigger is created in its place.</p><pre class="crayon-plain-tag">keith=# alter table jobmon.job_detail drop constraint job_detail_job_id_fkey;
ALTER TABLE

keith=# CREATE OR REPLACE FUNCTION jobmon.job_detail_fk_trigger() RETURNS trigger
keith-#     LANGUAGE plpgsql
keith-#     AS $$
keith$# DECLARE
keith$# v_job_id    bigint;
keith$# BEGIN
keith$#     SELECT l.job_id INTO v_job_id
keith$#     FROM jobmon.job_log l
keith$#     WHERE l.job_id = NEW.job_id;
keith$# 
keith$#     IF v_job_id IS NULL THEN
keith$#         RAISE foreign_key_violation USING 
keith$#             MESSAGE='Insert or update on table "jobmon.job_detail" violates custom foreign key trigger "job_detail_fk_trigger" ',
keith$#             DETAIL='Key (job_id='||NEW.job_id||') is not present in jobmon.job_log';
keith$#     END IF;
keith$#     RETURN NEW;
keith$# END
keith$# $$;
CREATE FUNCTION

keith=# 
keith=# 
keith=# CREATE TRIGGER aa_job_detail_fk_trigger 
keith-# BEFORE INSERT OR UPDATE OF job_id
keith-# ON jobmon.job_detail
keith-# FOR EACH ROW
keith-# EXECUTE PROCEDURE jobmon.job_detail_fk_trigger();
CREATE TRIGGER</pre><p>This MUST be a BEFORE trigger and I gave the trigger name a prefix of &#8220;aa_&#8221; because PostgreSQL fires triggers off in alphabetical order and I want to ensure it goes first as best I can.Â  Now we partition job_detail &amp; job_log the same as before.</p><pre class="crayon-plain-tag">keith=# select partman.create_parent('jobmon.job_detail', 'job_id', 'id-static', '10000', p_jobmon := false);
 create_parent 
---------------
 
(1 row)

Time: 168.831 ms
keith=# \d+ jobmon.job_detail
                                                               Table "jobmon.job_detail"
    Column    |           Type           |                              Modifiers                              | Storage  | Stats target | Description 
--------------+--------------------------+---------------------------------------------------------------------+----------+--------------+-------------
 job_id       | bigint                   | not null                                                            | plain    |              | 
 step_id      | bigint                   | not null default nextval('jobmon.job_detail_step_id_seq'::regclass) | plain    |              | 
 action       | text                     | not null                                                            | extended |              | 
 start_time   | timestamp with time zone | not null                                                            | plain    |              | 
 end_time     | timestamp with time zone |                                                                     | plain    |              | 
 elapsed_time | real                     |                                                                     | plain    |              | 
 status       | text                     |                                                                     | extended |              | 
 message      | text                     |                                                                     | extended |              | 
Indexes:
    "job_detail_step_id_pkey" PRIMARY KEY, btree (step_id)
    "job_detail_job_id_idx" btree (job_id)
Triggers:
    aa_job_detail_fk_trigger BEFORE INSERT OR UPDATE OF job_id ON jobmon.job_detail FOR EACH ROW EXECUTE PROCEDURE jobmon.job_detail_fk_trigger()
    job_detail_part_trig BEFORE INSERT ON jobmon.job_detail FOR EACH ROW EXECUTE PROCEDURE jobmon.job_detail_part_trig_func()
Child tables: jobmon.job_detail_p0,
              jobmon.job_detail_p10000,
              jobmon.job_detail_p20000,
              jobmon.job_detail_p30000,
              jobmon.job_detail_p40000
Has OIDs: no

keith=# \d+ jobmon.job_detail_p0
                                                             Table "jobmon.job_detail_p0"
    Column    |           Type           |                              Modifiers                              | Storage  | Stats target | Description 
--------------+--------------------------+---------------------------------------------------------------------+----------+--------------+-------------
 job_id       | bigint                   | not null                                                            | plain    |              | 
 step_id      | bigint                   | not null default nextval('jobmon.job_detail_step_id_seq'::regclass) | plain    |              | 
 action       | text                     | not null                                                            | extended |              | 
 start_time   | timestamp with time zone | not null                                                            | plain    |              | 
 end_time     | timestamp with time zone |                                                                     | plain    |              | 
 elapsed_time | real                     |                                                                     | plain    |              | 
 status       | text                     |                                                                     | extended |              | 
 message      | text                     |                                                                     | extended |              | 
Indexes:
    "job_detail_p0_pkey" PRIMARY KEY, btree (step_id)
    "job_detail_p0_job_id_idx" btree (job_id)
Check constraints:
    "job_detail_p0_partition_check" CHECK (job_id &gt;= 0::bigint AND job_id &lt; 10000::bigint)
Inherits: jobmon.job_detail
Has OIDs: no

keith=# select partman.create_parent('jobmon.job_log', 'job_id', 'id-static', '10000', p_jobmon := false);
 create_parent 
---------------
 
(1 row)

Time: 197.390 ms
keith=# \d+ jobmon.job_log
                                                             Table "jobmon.job_log"
   Column   |           Type           |                            Modifiers                            | Storage  | Stats target | Description 
------------+--------------------------+-----------------------------------------------------------------+----------+--------------+-------------
 job_id     | bigint                   | not null default nextval('jobmon.job_log_job_id_seq'::regclass) | plain    |              | 
 owner      | text                     | not null                                                        | extended |              | 
 job_name   | text                     | not null                                                        | extended |              | 
 start_time | timestamp with time zone | not null                                                        | plain    |              | 
 end_time   | timestamp with time zone |                                                                 | plain    |              | 
 status     | text                     |                                                                 | extended |              | 
 pid        | integer                  | not null                                                        | plain    |              | 
Indexes:
    "job_log_job_id_pkey" PRIMARY KEY, btree (job_id)
    "job_log_job_name_idx" btree (job_name)
    "job_log_pid_idx" btree (pid)
    "job_log_start_time_idx" btree (start_time)
    "job_log_status_idx" btree (status)
Triggers:
    job_log_part_trig BEFORE INSERT ON jobmon.job_log FOR EACH ROW EXECUTE PROCEDURE jobmon.job_log_part_trig_func()
    trg_job_monitor AFTER UPDATE ON jobmon.job_log FOR EACH ROW EXECUTE PROCEDURE jobmon.job_monitor()
Child tables: jobmon.job_log_p0,
              jobmon.job_log_p10000,
              jobmon.job_log_p20000,
              jobmon.job_log_p30000,
              jobmon.job_log_p40000
Has OIDs: no</pre><p>You can see that triggers are not inherited to child tables, so that is why it must be a BEFORE trigger on the job_detail parent. The insert does not actually happen on the job_detail parent table, so the event must be caught before any insert is actually done. Also, this isn&#8217;t quite as flexible as a real foreign key since there are no CASCADE options to handle data being removed on the parent. This also causes much heavier locks than a real foreign key. Lets see what happens if we try the same inserts that failed above</p><pre class="crayon-plain-tag">keith=# INSERT INTO jobmon.job_log (owner, job_name, start_time, pid) values ('keith', 'FK FAILURE TEST', now(), pg_backend_pid());
INSERT 0 0

keith=# select * from jobmon.job_log;
 job_id | owner |    job_name     |          start_time          | end_time | status | pid  
--------+-------+-----------------+------------------------------+----------+--------+------
      1 | keith | FK FAILURE TEST | 2014-05-27 12:59:03.06901-04 | Â«NULLÂ»   | Â«NULLÂ» | 3591
(1 row)

keith=# insert into jobmon.job_detail (job_id, action, start_time) values (1, 'FK FAILURE TEST STEP 1', now());
INSERT 0 0

keith=# select * from jobmon.job_detail;
 job_id | step_id |         action         |          start_time          | end_time | elapsed_time | status | message 
--------+---------+------------------------+------------------------------+----------+--------------+--------+---------
      1 |       1 | FK FAILURE TEST STEP 1 | 2014-05-27 12:59:40.03766-04 | Â«NULLÂ»   |       Â«NULLÂ» | Â«NULLÂ» | Â«NULLÂ»
(1 row)

keith=# select * from only jobmon.job_detail;
 job_id | step_id | action | start_time | end_time | elapsed_time | status | message 
--------+---------+--------+------------+----------+--------------+--------+---------
(0 rows)

keith=# select * from only jobmon.job_detail_p0;
 job_id | step_id |         action         |          start_time          | end_time | elapsed_time | status | message 
--------+---------+------------------------+------------------------------+----------+--------------+--------+---------
      1 |       1 | FK FAILURE TEST STEP 1 | 2014-05-27 12:59:40.03766-04 | Â«NULLÂ»   |       Â«NULLÂ» | Â«NULLÂ» | Â«NULLÂ»
(1 row)</pre><p>No errors! And what happens if we try and insert invalid data to the child table?</p><pre class="crayon-plain-tag">keith=# insert into jobmon.job_detail (job_id, action, start_time) values (2, 'FK FAILURE TEST STEP 1', now());
ERROR:  Insert or update on table "jobmon.job_detail" violates custom foreign key trigger "job_detail_fk_trigger" 
DETAIL:  Key (job_id=2) is not present in jobmon.job_log</pre><p>Since the trigger function is doing a normal select on the parent table of the job_log partition set, it is seeing data across all the child partitions. AND, since job_id is the partition column of job_log, the trigger function will actually be able to take advantage of constraint exclusion and will only actually touch the one single partition that value could be in. So this works very well in this case, even if the partition set grows extremely large. Now, if you create a FK trigger like this on any other column that doesn&#8217;t have constraints, you will begin to notice performance issues as the reference table grows in size. If your tables contain static, unchanging data, pg_partman has some additional options that can help here as well (see my <a href="http://www.keithf4.com/managing-constraint-exclusion-in-table-partitioning/">previous post</a> about constraint exclusion).</p>
<p>The other issue with this is exclusive to pg_jobmon being an extension. The lack of a foreign key and presence of a trigger is different than the default extension code. There is the potential that a future extension update could either remove the trigger or replace the foreign key. There&#8217;s currently no way to give extension installation options for different code branches that I&#8217;m aware of and keep things consistent. In the case of pg_jobmon, the extension is mostly feature complete and I don&#8217;t foresee any updates breaking the above fix. But it is something to be aware of if you have to change the default code in any extension.</p>
<p>This is a complicated issue and one that many people don&#8217;t realize when trying to plan out table partitioning for more complex schemas. Hopefully I&#8217;ve helped clarify things and shown why partitioning is such a tricky issue to get right.</p>
    </div>
    <div class="feedEntry">

        <a href="http://bonesmoses.org/2014/05/27/pgcon-2014-unconference-a-community/" title="Shaun M. Thomas: PGCon 2014 Unconference: A Community">
            <h2>Shaun M. Thomas: PGCon 2014 Unconference: A Community</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">May 27, 2014</span>.
            
        </p>

        <p>This May, I attended my first international conference: <a href="http://www.pgcon.org/2014/">PGCon 2014</a>. Though the schedule spanned from May 20th to May 23rd, I came primarily for the talks. Then there was the Unconference on the 24th. I&#8217;d never heard of such a thing, but it was billed as a good way to network and find out what community members want from PostgreSQL. After attending the Unconference, I must admit I&#8217;m exceptionally glad it exists; it&#8217;s something I believe every strong Open Source project needs.</p>

<p>Why do I say that, having only been to one of them? It&#8217;s actually fairly simple. Around 10AM Saturday, everyone piled into the large lecture hall and had a seat. There were significantly fewer attendees, but most of the core committers remained for the festivities. We were promised pizza and PostgreSQL, and that&#8217;s all anyone needed. <a href="http://www.pgexperts.com/team/josh_berkus/">Josh Berkus </a> started the festivities by announcing the rules and polling for ideas. The <a href="https://wiki.postgresql.org/wiki/Pgcon2014unconference">final schedule</a> was pretty interesting in itself, but I was more enamored by the process and the response it elicited.</p>

<p>I&#8217;m no stranger to the community, and the <a href="http://www.postgresql.org/list/">mailing lists</a> are almost overwhelmingly active. But these conversations, nearly all of them, are focused on assistance and hacker background noise. The thing that stood out to me during the Unconference planning was its organic nature. It wasn&#8217;t just that we chose the event schedule democratically. It wasn&#8217;t the wide range of topics. It wasn&#8217;t even the fact core members were there to listen. It was the engagement.</p>

<p>These people were <em>excited</em> and enjoying talking about PostgreSQL in a way I&#8217;ve never witnessed, and I&#8217;ve spoken at <a href="http://postgresopen.org/2014/">Postgres Open</a> twice so far. I&#8217;ve seen several talks, been on both sides of the podium, and no previous experience even comes close. We were all <em>having fun</em> brainstorming about PostgreSQL and its future. For one day, it wasn&#8217;t about pre-cooked presentations chosen via committee, but about what &#8220;the community&#8221; wanted to discuss.</p>

<p>When it came time for the talks themselves, this atmosphere persisted. We agreed and disagreed, we had long and concise arguments for and against ideas, clarified our positions, and <em>generally</em> converged toward a loose consensus. And it was glorious. I know we were recording the sessions, so if you have the time when the videos are available, I urge you to watch just one so you can see the beauty and flow of our conversations.</p>

<p>I feel so strongly about this that I believe PGCon needs to start a day earlier. One unfortunate element about the Unconference is that it happens on a Saturday, when everyone wants to leave and return to their families. Worse, there is a marathon on Sunday, meaning it is difficult or even impossible to secure a hotel room for the Saturday event. People tend to follow the path of least resistance, so if there is a problem getting lodging, they won&#8217;t go.</p>

<p>And that&#8217;s a shame. Having a core of interested and engaged community members not only improves the reputation of PostgreSQL, but its advocacy as well. If people feel they can contribute without having to code, they&#8217;ll be more likely to do so. If those contributions, no matter how small, are acknowledged, their progenitors will stick around. I believe this is the grass-roots effort that makes PostgreSQL the future of the database world, and whoever came up with the Unconference deserves every accolade I can exclaim.</p>

<p>We need more of this. PostgreSQL has one of the most open communities I&#8217;ve had the pleasure of participating in, and that kind of momentum can&#8217;t really be forced. I hope every future PostgreSQL conference in every country has one of these, so everyone with the motivation can take part in the process.</p>

<p>Finally, find your nearest <a href="http://www.postgresql.org/community/user-groups/">PostgreSQL User Group</a>, and join the community. You&#8217;ll be glad you did.</p>
    </div>
    <div class="feedEntry">

        <a href="http://www.databasesoup.com/2014/05/94-theme-contest-analyzed-by-94.html" title="Josh Berkus: 9.4 Theme Contest Analyzed by 9.4">
            <h2>Josh Berkus: 9.4 Theme Contest Analyzed by 9.4</h2>
        </a>

        <p class="discreet">
            
            
                  From Planet PostgreSQL.
            
            
                Published on <span name="publication_time">May 25, 2014</span>.
            
        </p>

        So a couple weeks ago I ran a little contest to see who could come up with a slogan for PostgreSQL 9.4.&nbsp; Surprisingly, we got over 300 votes on various slogans, which means I need to do some statistics to analyze them -- which means I'm going to show off some of PostgreSQL's new 9.4 features as part of that!<br /><br />Version 9.4 includes a number of new aggregate, array and set operations which make it vastly easier and faster to do statistical summaries and analysis.&nbsp; Most of these were contributed by Andrew Gierth, including the two I'm going to use below, FILTER and WITHIN GROUP.&nbsp; I'm also going to use MATERIALIZED VIEWS, developed by Kevin Grittner.&nbsp; First, though, I need to import the data.&nbsp; So I downloaded the survey results as a CSV, and created a table for them in PostgreSQL and loaded it up:<br /><br /><span>CREATE TABLE raw_survey (<br />&nbsp;&nbsp;&nbsp; ts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; timestamptz,<br />&nbsp;&nbsp;&nbsp; prf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; integer,<br />&nbsp;&nbsp;&nbsp; moreways integer,<br />&nbsp;&nbsp;&nbsp; devops&nbsp;&nbsp; integer,<br />&nbsp;&nbsp;&nbsp; moresql&nbsp; integer,<br />&nbsp;&nbsp;&nbsp; yesql&nbsp;&nbsp;&nbsp; integer,<br />);</span><br /><span>\copy raw_survey from 'slogans.csv' with csv header</span><br /><br />Now, Google's column-per-question format isn't very friendly to analysis and comparison; I want a more vertical orientation.&nbsp; So I create one as a MatView.&nbsp; This means that if I reimport the data in the future, or weed out obvious ballot-box stuffing, I just need to refresh it:<br /><br /><span>CREATE MATERIALIZED VIEW slogans AS<br />SELECT 'Performance, Reliability, Flexibility' as slogan, prf as vote<br />FROM raw_survey<br />UNION ALL<br />SELECT 'More Ways to Database', moreways<br />FROM raw_survey<br />UNION ALL<br />SELECT 'A Database for Dev and Ops', devops<br />FROM raw_survey<br />UNION ALL<br />SELECT 'More Than SQL', moresql<br />FROM raw_survey<br />UNION ALL<br />SELECT 'NoSQL + YeSQL = PostgreSQL', yesql<br />FROM raw_survey;</span><br /><br />Now, for some statistics.&nbsp; A total or average is easy, but it's not statistically sound.&nbsp; A median is a much better statistic.&nbsp; I also want to know the balance of people who hated a slogan (1) vs. loved it and put it first (5).&nbsp; So, some of the new aggregates.<br /><br />In the past, I've retrieved medians by either using SciPy inside PL/Python, or by doing some elaborate calculations on windowing rank.&nbsp; No more.&nbsp; Now I can do a simple one-line median using WITHIN GROUP.&nbsp; WITHIN GROUP is a lot like a windowing aggregate, except that it returns a single summary aggregate.&nbsp; Shipping with version 9.4 are several such aggregates, including percentile_cont() which is one of three functions which allow you to get the value at the stated percent of a sorted group: in this case, 0.5 to get a median.&nbsp; Like so:<br /><br /><span>SELECT slogan,<br />&nbsp;&nbsp;&nbsp; percentile_cont(0.5) WITHIN GROUP (ORDER BY vote)<br />FROM slogans<br />GROUP BY slogan;</span><br /><br /><table border="0">  <tbody><tr>    <th align="center">slogan</th>    <th align="center">percentile_cont</th>  </tr><tr valign="top">    <td align="left">A Database for Dev and Ops</td>    <td align="right">3</td>  </tr><tr valign="top">    <td align="left">More Than SQL</td>    <td align="right">3</td>  </tr><tr valign="top">    <td align="left">More Ways to Database</td>    <td align="right">3</td>  </tr><tr valign="top">    <td align="left">NoSQL + YeSQL = PostgreSQL</td>    <td align="right">3</td>  </tr><tr valign="top">    <td align="left">Performance, Reliability, Flexibility</td>    <td align="right">4</td>  </tr></tbody></table><br />"Performance, Reliability, Flexibility" is taking a clear lead here.&nbsp; Incidentally, percentile_cont() can take an array of values in order to give you a full box (remember, every time you say "big data" without drawing a box plot, God kills a kitten):<br /><br /><span>SELECT slogan,<br />&nbsp;&nbsp;&nbsp; percentile_cont(ARRAY[0.1,0.25,0.5,0.75,0.9]) WITHIN GROUP (ORDER BY vote)<br />FROM slogans<br />GROUP BY slogan;</span><br /><br /><table border="0">  <tbody><tr>    <th align="center">slogan</th>    <th align="center">percentile_cont</th>  </tr><tr valign="top">    <td align="left">A Database for Dev and Ops</td>    <td align="left">{1,2,3,3,4}</td>  </tr><tr valign="top">    <td align="left">More Than SQL</td>    <td align="left">{1.4,2,3,4,5}</td>  </tr><tr valign="top">      <td align="left">More Ways to Database</td>    <td align="left">{1,2,3,4,5}</td>  </tr><tr valign="top">    <td align="left">NoSQL + YeSQL = PostgreSQL</td>    <td align="left">{1,1,3,4,5}</td>  </tr><tr valign="top">    <td align="left">Performance, Reliability, Flexibility</td>    <td align="left">{2,3,4,5,5}</td>  </tr></tbody></table>Let's check or "loves" and "hates" to see if they tell us anything different.&nbsp; Now, the old way to do this would be:<br /><br /><span>SELECT slogan,<br />&nbsp;&nbsp;&nbsp; sum(CASE WHEN vote = 1 THEN 1 ELSE 0 END) as hates,<br />&nbsp;&nbsp;&nbsp; sum(CASE WHEN vote = 5 THEN 1 ELSE 0 END) as loves<br />FROM slogans<br />GROUP BY slogan;</span><br /><br />Awkward, neh?&nbsp; Well, no more, thanks to the FILTER clause:<br /><br /><span>SELECT slogan,<br />&nbsp;&nbsp;&nbsp; count(*) FILTER ( WHERE vote = 1 ) as hates,<br />&nbsp;&nbsp;&nbsp; count(*) FILTER ( WHERE vote = 1 ) as loves<br />FROM slogans<br />GROUP BY slogan;</span><br /><br />Isn't that way more intuitive and readable?&nbsp; I think it is, anyway.&nbsp; So, let's put it all together:<br /><br /><span>SELECT slogan,<br />&nbsp;&nbsp;&nbsp; percentile_cont(0.5) WITHIN GROUP (ORDER BY vote) as median,<br />&nbsp;&nbsp;&nbsp; count(*) FILTER ( WHERE vote = 1 ) as hates,<br />&nbsp;&nbsp;&nbsp; count(*) FILTER ( WHERE vote = 5 ) as loves<br />FROM slogans<br />GROUP BY slogan;</span><br /><br />And the results:<br /><br /><br /><table border="0">  <tbody><tr>    <th align="center">slogan</th>    <th align="center">median</th>    <th align="center">hates</th>    <th align="center">loves</th>  </tr><tr valign="top">    <td align="left">A Database for Dev and Ops</td>    <td align="right">3</td>    <td align="right">47</td>    <td align="right">21</td>  </tr><tr valign="top">    <td align="left">More Than SQL</td>    <td align="right">3</td>    <td align="right">32</td>    <td align="right">58</td>  </tr><tr valign="top">    <td align="left">More Ways to Database</td>    <td align="right">3</td>    <td align="right">39</td>    <td align="right">55</td>  </tr><tr valign="top">    <td align="left">NoSQL + YeSQL = PostgreSQL</td>    <td align="right">3</td>    <td align="right">81</td>    <td align="right">58</td>  </tr><tr valign="top">    <td align="left">Performance, Reliability, Flexibility</td>    <td align="right">4</td>    <td align="right">11</td>    <td align="right">118</td>  </tr></tbody></table><br />And there we have it: "Performance, Reliability, Flexibility" is the winning theme idea for 9.4.&nbsp; It wins on median, and on hates vs. loves counts.<br /><br />Congratulations Korry Douglas; I'll contact you about shipping your Chelnik.&nbsp; Note that the theme will be workshopped a little bit to fit in the structure of the final 9.4 release announcement (i.e. we may change it slightly to match the sections of the actual press release), but we're going with that general idea now.
    </div>
    <div class="feedEntry">

        <a href="http://www.planetdjango.org/#shutting-down" title="PlanetDjango.org shutting down">
            <h2>PlanetDjango.org shutting down</h2>
        </a>

        <p class="discreet">
            
                  By Adomas PaltanaviÄius from Planet Django.
            
            
            
                Published on <span name="publication_time">Mar 08, 2014</span>.
            
        </p>

        
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/is_open_source_consulting_dead" title="Is Open Source Consulting Dead?">
            <h2>Is Open Source Consulting Dead?</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Sep 10, 2013</span>.
            
        </p>

        Has Elvis left the building?  Will we be able to sustain ourselves as open source consultants?
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/consulting_and_patent_indemnification" title="Consulting and Patent Indemification">
            <h2>Consulting and Patent Indemification</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Aug 09, 2013</span>.
            
        </p>

        Article about consulting and patent indemnification
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/python_advent_2012" title="Python Advent Calendar 2012 Topic">
            <h2>Python Advent Calendar 2012 Topic</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Dec 24, 2012</span>.
            
        </p>

        An entry for the 2012 Japanese advent calendar at http://connpass.com/event/1439/
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/why_i_like_zodb" title="Why I Like ZODB">
            <h2>Why I Like ZODB</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">May 15, 2012</span>.
            
        </p>

        Why I like ZODB better than other persistence systems for writing real-world web applications.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/python_2_vs_python_3_str_iter" title="A str. __iter__ Gotcha in Cross-Compatible Py2/Py3 Code">
            <h2>A str. __iter__ Gotcha in Cross-Compatible Py2/Py3 Code</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Mar 03, 2012</span>.
            
        </p>

        A bug caused by a minor incompatibility can remain latent for long periods of time in a cross-compatible Python 2 / Python 3 codebase.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/in_praise_of_complaining" title="In Praise of Complaining">
            <h2>In Praise of Complaining</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Jan 01, 2012</span>.
            
        </p>

        In praise of complaining, even when the complaints are absurd.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/2012_python_meme" title="2012 Python Meme">
            <h2>2012 Python Meme</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Dec 24, 2011</span>.
            
        </p>

        My "Python meme" replies.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/in_defense_of_zope_libraries" title="In Defense of Zope Libraries">
            <h2>In Defense of Zope Libraries</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Dec 19, 2011</span>.
            
        </p>

        A much too long defense of Pyramid's use of Zope libraries.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/ploneconf_2011_pyramid_sprint" title="Plone Conference 2011 Pyramid Sprint">
            <h2>Plone Conference 2011 Pyramid Sprint</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Nov 10, 2011</span>.
            
        </p>

        An update about the happenings at the recent 2011 Plone Conference Pyramid sprint.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/oss_jobsification" title="Jobs-Ification of Software Development">
            <h2>Jobs-Ification of Software Development</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Oct 17, 2011</span>.
            
        </p>

        Try not to Jobs-ify the task of software development.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/webob_py3" title="WebOb Now on Python 3">
            <h2>WebOb Now on Python 3</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Oct 15, 2011</span>.
            
        </p>

        Report about porting to Python 3.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/oss_sarcasm" title="Open Source Project Maintainer Sarcastic Response Cheat Sheet">
            <h2>Open Source Project Maintainer Sarcastic Response Cheat Sheet</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Jun 12, 2011</span>.
            
        </p>

        Need a sarcastic response to a support interaction as an open source project maintainer?  Look no further!
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/minicon0_wrapup" title="Pylons Miniconference #0 Wrapup">
            <h2>Pylons Miniconference #0 Wrapup</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">May 04, 2011</span>.
            
        </p>

        Last week, I visited the lovely Bay Area to attend the 0th Pylons
Miniconference in San Francisco.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/pylons_project_minicon" title="Pylons Project Meetup / Minicon">
            <h2>Pylons Project Meetup / Minicon</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Apr 14, 2011</span>.
            
        </p>

        In the SF Bay Area on the 28th, 29th, and 30th of this month (April), 3 separate Pylons Project events.
    </div>
    <div class="feedEntry">

        <a href="http://plope.com/Members/chrism/pycon_2011" title="PyCon 2011 Report">
            <h2>PyCon 2011 Report</h2>
        </a>

        <p class="discreet">
            
                  By chrism from plope.
            
            
            
                Published on <span name="publication_time">Mar 19, 2011</span>.
            
        </p>

        My personal PyCon 2011 Report
    </div>

    

    <div class="visualClear"><!-- --></div>

    <div class="documentActions">
        

        

    </div>


                        </div>
                    

                    
                </div>
            

            <div id="viewlet-below-content">

<div id="portlets-below" class="row">
     
         
             <div class="cell BelowPortletManager2 width-1:2 position-0">


<div id="portletwrapper-436f6e74656e7457656c6c506f72746c6574732e42656c6f77506f72746c65744d616e61676572320a636f6e746578740a2f506c6f6e652f636f7665720a6c61746573742d706c6f6e652d706f737473" class="portletWrapper kssattr-portlethash-436f6e74656e7457656c6c506f72746c6574732e42656c6f77506f72746c65744d616e61676572320a636f6e746578740a2f506c6f6e652f636f7665720a6c61746573742d706c6f6e652d706f737473"><dl class="portlet portletfeedmixer">

    <dt class="portletHeader">
        <span class="portletTopLeft"></span>
        <span>Latest Plone Posts</span>
        <span class="portletTopRight"></span>
    </dt>

    
        <dd class="portletItem odd">

            <a href="http://feeds.plone.org/~r/ploneblogs/~3/Yvkwx-UncaI/plone-mosaic-sprint-final-report" title="Plone Mosaic sprint final report">
                Plone Mosaic sprint final report
                <span class="portletItemDetails">Jun 16, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem even">

            <a href="http://feeds.plone.org/~r/ploneblogs/~3/C7bxaFp37Nc/tips.html" title="Mail tips to your colleagues">
                Mail tips to your colleagues
                <span class="portletItemDetails">Jun 13, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem odd">

            <a href="http://feeds.plone.org/~r/ploneblogs/~3/vAVkdPdv84c/plone-intranet-development-sprint-update" title="Plone Intranet Development Sprint update (May 2014)">
                Plone Intranet Development Sprint update (May 2014)
                <span class="portletItemDetails">Jun 12, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem even">

            <a href="http://feeds.plone.org/~r/ploneblogs/~3/8vJfSh2LzUc/a-few-notes-from-the-plone-mosaic-sprint" title="A few notes from the Plone Mosaic Sprint">
                A few notes from the Plone Mosaic Sprint
                <span class="portletItemDetails">Jun 11, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem odd">

            <a href="http://feeds.plone.org/~r/ploneblogs/~3/HIiGvbWWX1A/20-juni-four-o-four-lightning-talks" title="20 juni: Four O Four Lightning Talks">
                20 juni: Four O Four Lightning Talks
                <span class="portletItemDetails">Jun 11, 2014</span>
            </a>
        </dd>
    
    
    <dd class="portletFooter">
      <a href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager2/latest-plone-posts/full_feed" class="tile">More&hellip;</a>
        <span class="portletBottomLeft"></span>
        <span class="portletBottomRight"></span>
    </dd>

</dl>
</div>

</div> 

         
         
             <div class="cell BelowPortletManager3 width-1:2 position-1:2">


<div id="portletwrapper-436f6e74656e7457656c6c506f72746c6574732e42656c6f77506f72746c65744d616e61676572330a636f6e746578740a2f506c6f6e652f636f7665720a6f70656e2d736f757263652d706f737473" class="portletWrapper kssattr-portlethash-436f6e74656e7457656c6c506f72746c6574732e42656c6f77506f72746c65744d616e61676572330a636f6e746578740a2f506c6f6e652f636f7665720a6f70656e2d736f757263652d706f737473"><dl class="portlet portletfeedmixer">

    <dt class="portletHeader">
        <span class="portletTopLeft"></span>
        <span>Open Source Posts</span>
        <span class="portletTopRight"></span>
    </dt>

    
        <dd class="portletItem odd">

            <a href="http://blog.2ndquadrant.com/postgresql-no-tablespaces-on-ramdisks/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=postgresql-no-tablespaces-on-ramdisks" title="Craig Ringer: Putting a PostgreSQL tablespace on a ramdisk risks ALL your data">
                Craig Ringer: Putting a PostgreSQL tablespace on a ramdisk risks ALL your data
                <span class="portletItemDetails">Jun 15, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem even">

            <a href="http://adpgtech.blogspot.com/2014/06/json-in-postrgesql-94-video.html" title="Andrew Dunstan: JSON in PostgreSQL 9.4 Video">
                Andrew Dunstan: JSON in PostgreSQL 9.4 Video
                <span class="portletItemDetails">Jun 15, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem odd">

            <a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-gist-inet-datatype/" title="Michael Paquier: Postgres 9.4 feature highlight: GiST operator class for inet and cidr datatypes">
                Michael Paquier: Postgres 9.4 feature highlight: GiST operator class for inet and cidr datatypes
                <span class="portletItemDetails">Jun 15, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem even">

            <a href="http://momjian.us/main/blogs/pgblog/2014.html#June_14_2014" title="Bruce Momjian: Postgres Pool Party">
                Bruce Momjian: Postgres Pool Party
                <span class="portletItemDetails">Jun 14, 2014</span>
            </a>
        </dd>
    
    
        <dd class="portletItem odd">

            <a href="http://pgsnaga.blogspot.com/2014/06/deploying-postgres-xl-in-2-minutes-with.html" title="Satoshi Nagayasu: Deploying Postgres-XL in 2-minutes with Chef/serverspec">
                Satoshi Nagayasu: Deploying Postgres-XL in 2-minutes with Chef/serverspec
                <span class="portletItemDetails">Jun 14, 2014</span>
            </a>
        </dd>
    
    
    <dd class="portletFooter">
      <a href="http://crisewing.com/cover/++contextportlets++ContentWellPortlets.BelowPortletManager3/open-source-posts/full_feed" class="tile">More&hellip;</a>
        <span class="portletBottomLeft"></span>
        <span class="portletBottomRight"></span>
    </dd>

</dl>
</div>

</div> 

         
     
</div>


</div>
        </div>

        
        

        
        
    </div>


    <div class="row">
        <div id="portlets-footer" class="row">
     
     
</div>



<div class="row">
    <div class="cell width-full position-0">
        <div id="portal-footer">
          <a id="license-img" rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">
            <img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" />
          </a>
          <p>
            This site and its content is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Creative
              Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>, 
              2011&mdash;2014
          </p>
          <div class="visualClear"><!-- clear floats --></div>
        </div>

    </div>
</div>
<div id="portal-colophon">

<div class="colophonWrapper">
<ul>
  <li>
    <a href="http://plone.org" title="This site was built using the Plone Open Source CMS/WCM.">
      Powered by Plone &amp; Python</a>
  </li>
</ul>
</div>
</div>

<ul id="portal-siteactions">

    <li id="siteaction-sitemap"><a href="http://crisewing.com/sitemap" accesskey="3" title="Site Map">Site Map</a></li>
    <li id="siteaction-accessibility"><a href="http://crisewing.com/accessibility-info" accesskey="0" title="Accessibility">Accessibility</a></li>
    <li id="siteaction-contact"><a href="http://crisewing.com/about/contact" accesskey="9" title="Contact">Contact</a></li>
</ul>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22486368-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
        <div id="kss-spinner">
            <img alt="" src="http://crisewing.com/spinner.gif" />
        </div>
    </div>

</div>
</body>
</html>



